{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1a4c4a0",
   "metadata": {},
   "source": [
    "# Using LLMs and Knowledge graphs to search for PFAS Alternatives\n",
    "\n",
    "## Project with Saint Gobain\n",
    "\n",
    "#### Yu-Chuan (Michael) Hsu, Isabella Stewart, Tarjei Hage, Wei Lu, and Markus J. Buehler, MIT, 2025 \n",
    "mkychsu@MIT.EDU, istewart@MIT.EDU, tphage@MIT.EDU, wl7@MIT.EDU, mbuehler@MIT.EDU\n",
    "#### LAMM, Massachusetts Institute of Technology\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c2660-bd21-4c34-a4b4-748f469d8925",
   "metadata": {},
   "source": [
    "# Allows for distributed or parallel processing of a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ebbae2c-62eb-4878-a45f-2f305b16bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "try:\n",
    "    thread_i = int(sys.argv[1]) #which thread number this process is (e.g., in multi-threaded runs)\n",
    "    total_threads = int(sys.argv[2]) #how many total threads are running\n",
    "\n",
    "except: \n",
    "    thread_i = 0 #If no arguments are provided (e.g. during a notebook run), it defaults to a single-threaded run\n",
    "    total_threads = 1\n",
    "    merge_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85741c5c-72ff-477e-928e-08d4e04cad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%env TOGETHER_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f7bb95-6eeb-42c9-89f5-7dfdf39a6d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.getenv(\"TOGETHER_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed55ce76-958f-4b5d-bcfd-73d326272df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = [\n",
    "    {\n",
    "        \"model\":\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "        # \"model\":\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "        # \"model\":\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "        \n",
    "        \"api_key\":os.getenv(\"TOGETHER_API_KEY\"),\n",
    "        \"max_tokens\": 20000\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e914d-94b2-4cfe-9f0a-a4c17ec88991",
   "metadata": {},
   "source": [
    "# Client Initiation with Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40fca58b-ec17-414a-aecb-bc5d1a779609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "client = Together(api_key=config_list[0][\"api_key\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "336e744c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#from GraphReasoning import *\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(0, '/home/istewart/pool/saintgobain/GraphReasoning_SG') #change functions here. \n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/orcd/home/002/istewart/orcd/pool/hypergraph/GraphReasoning_SG') #change functions here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ee71b51-cfa6-4dca-aa55-de6b48f19c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbatim=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac05b174-03ef-4443-b25b-fb571a23c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_data_dir = './CompositePDFs_marker' #place where you keep your markdown files\n",
    "data_dir='./GRAPHDATA_paper'    \n",
    "data_dir_output='./GRAPHDATA_OUTPUT_paper'\n",
    "\n",
    "max_tokens = config_list[0]['max_tokens']\n",
    "\n",
    "embedding_file='composite_LLAMA4_70b.pkl' #what your embedding file will be \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac48b9f9-0882-4fca-b69f-51887ed7f1d0",
   "metadata": {},
   "source": [
    "# Embedding the graph with Nomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7369be73-4ae6-44d9-a779-22efb43d704d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/istewart/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_threads \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;66;03m##merging mode: This block only runs if you're not distributing work across multiple threads.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# from tqdm.notebook import tqdm\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# from IPython.display import display, Markdown\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# embedding_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model,use_fast=False, device_map=\"cuda:0\")\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# embedding_model = AutoModelForCausalLM.from_pretrained(tokenizer_model,output_hidden_states=True).to('cuda:0')\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     logging,\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/transformers/dependency_versions_check.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[1;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/transformers/utils/__init__.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     add_code_sample_docstrings,\n\u001b[1;32m     31\u001b[0m     add_end_docstrings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     replace_return_docstrings,\n\u001b[1;32m     36\u001b[0m )\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/transformers/utils/chat_template_utils.py:40\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m     43\u001b[0m BASIC_TYPES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Any, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Extracts the initial segment of the docstring, containing the function description\u001b[39;00m\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/torch/__init__.py:2604\u001b[0m\n\u001b[1;32m   2600\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vmap \u001b[38;5;28;01mas\u001b[39;00m vmap\n\u001b[1;32m   2603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m-> 2604\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[1;32m   2606\u001b[0m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTORCH_CUDA_SANITIZER\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/torch/_meta_registrations.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SymBool, SymFloat, Tensor\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     _add_op_to_registry,\n\u001b[1;32m     13\u001b[0m     _convert_out_params,\n\u001b[1;32m     14\u001b[0m     global_decomposition_table,\n\u001b[1;32m     15\u001b[0m     meta_table,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _prim_elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/torch/_decomp/__init__.py:284\u001b[0m\n\u001b[1;32m    280\u001b[0m             decompositions\u001b[38;5;241m.\u001b[39mpop(op, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# populate the table\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecompositions\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_refs\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcore_aten_decompositions\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomDecompTable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/torch/_decomp/decompositions.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_meta_registrations\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprims\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/torch/_prims/__init__.py:524\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# Elementwise unary operations\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m \u001b[38;5;28mabs\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_make_elementwise_unary_prim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimpl_aten\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOMPLEX_TO_FLOAT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m acos \u001b[38;5;241m=\u001b[39m _make_elementwise_unary_prim(\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macos\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    533\u001b[0m     impl_aten\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39macos,\n\u001b[1;32m    534\u001b[0m     doc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    535\u001b[0m     type_promotion\u001b[38;5;241m=\u001b[39mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[38;5;241m.\u001b[39mDEFAULT,\n\u001b[1;32m    536\u001b[0m )\n\u001b[1;32m    538\u001b[0m acosh \u001b[38;5;241m=\u001b[39m _make_elementwise_unary_prim(\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macosh\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    540\u001b[0m     impl_aten\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39macosh,\n\u001b[1;32m    541\u001b[0m     doc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    542\u001b[0m     type_promotion\u001b[38;5;241m=\u001b[39mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[38;5;241m.\u001b[39mDEFAULT,\n\u001b[1;32m    543\u001b[0m )\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/torch/_prims/__init__.py:492\u001b[0m, in \u001b[0;36m_make_elementwise_unary_prim\u001b[0;34m(name, type_promotion, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_make_elementwise_unary_prim\u001b[39m(\n\u001b[1;32m    486\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, type_promotion: ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    487\u001b[0m ):\n\u001b[1;32m    488\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03m    Creates an elementwise unary prim.\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_make_prim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m(Tensor self) -> Tensor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_prim_elementwise_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_promotion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRETURN_TYPE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNEW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/torch/_prims/__init__.py:320\u001b[0m, in \u001b[0;36m_make_prim\u001b[0;34m(schema, return_type, meta, impl_aten, doc, tags, use_old_custom_ops_api, register_conj_neg_fallthrough)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     mutates_args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    316\u001b[0m         arg\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m cpp_schema\u001b[38;5;241m.\u001b[39marguments\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m arg\u001b[38;5;241m.\u001b[39malias_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m arg\u001b[38;5;241m.\u001b[39malias_info\u001b[38;5;241m.\u001b[39mis_write\n\u001b[1;32m    319\u001b[0m     ]\n\u001b[0;32m--> 320\u001b[0m     prim_def \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprims::\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_prim_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutates_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmutates_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     prim_def\u001b[38;5;241m.\u001b[39mregister_fake(meta)\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;66;03m# all view ops get conj/neg fallthroughs\u001b[39;00m\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/torch/_library/custom_ops.py:145\u001b[0m, in \u001b[0;36mcustom_op\u001b[0;34m(name, fn, mutates_args, device_types, schema)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/torch/_library/custom_ops.py:126\u001b[0m, in \u001b[0;36mcustom_op.<locals>.inner\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    123\u001b[0m     schema_str \u001b[38;5;241m=\u001b[39m schema\n\u001b[1;32m    125\u001b[0m namespace, opname \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mCustomOpDef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Check that schema's alias annotations match those of `mutates_args`.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     expected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/torch/_library/custom_ops.py:174\u001b[0m, in \u001b[0;36mCustomOpDef.__init__\u001b[0;34m(self, namespace, name, schema, fn)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vmap_fn: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib \u001b[38;5;241m=\u001b[39m get_library_allowing_overwrite(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_namespace, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_to_dispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disabled_kernel: Set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    176\u001b[0m OPDEFS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qualname] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/torch/_library/custom_ops.py:594\u001b[0m, in \u001b[0;36mCustomOpDef._register_to_dispatcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    587\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was no fake impl registered for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is necessary for torch.compile/export/fx tracing to work. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.register_fake` to add an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake impl.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    591\u001b[0m         )\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_abstract_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 594\u001b[0m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_fake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m autograd_impl \u001b[38;5;241m=\u001b[39m autograd\u001b[38;5;241m.\u001b[39mmake_autograd_impl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opoverload, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    597\u001b[0m lib\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, autograd_impl, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutograd\u001b[39m\u001b[38;5;124m\"\u001b[39m, with_keyset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/torch/library.py:177\u001b[0m, in \u001b[0;36mLibrary._register_fake\u001b[0;34m(self, op_name, fn, _stacklevel)\u001b[0m\n\u001b[1;32m    174\u001b[0m     _library\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mwarn_deploy()\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_library\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m frame \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe(_stacklevel)\n\u001b[1;32m    179\u001b[0m caller_module \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(frame)\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/torch/_library/utils.py:53\u001b[0m, in \u001b[0;36mget_source\u001b[0;34m(stacklevel)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_source\u001b[39m(stacklevel: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a string that represents the caller.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    Example: \"/path/to/foo.py:42\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    etc.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetframeinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe\u001b[38;5;241m.\u001b[39mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe\u001b[38;5;241m.\u001b[39mlineno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m source\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/inspect.py:1624\u001b[0m, in \u001b[0;36mgetframeinfo\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1622\u001b[0m start \u001b[38;5;241m=\u001b[39m lineno \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m context\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1626\u001b[0m     lines \u001b[38;5;241m=\u001b[39m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/inspect.py:952\u001b[0m, in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (file\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m    950\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource code not available\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 952\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mgetmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module:\n\u001b[1;32m    954\u001b[0m     lines \u001b[38;5;241m=\u001b[39m linecache\u001b[38;5;241m.\u001b[39mgetlines(file, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/inspect.py:878\u001b[0m, in \u001b[0;36mgetmodule\u001b[0;34m(object, _filename)\u001b[0m\n\u001b[1;32m    875\u001b[0m         f \u001b[38;5;241m=\u001b[39m getabsfile(module)\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;66;03m# Always map to the name the module knows itself by\u001b[39;00m\n\u001b[1;32m    877\u001b[0m         modulesbyfile[f] \u001b[38;5;241m=\u001b[39m modulesbyfile[\n\u001b[0;32m--> 878\u001b[0m             \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m modulesbyfile:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mget(modulesbyfile[file])\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/posixpath.py:396\u001b[0m, in \u001b[0;36mrealpath\u001b[0;34m(filename, strict)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the canonical path of the specified filename, eliminating any\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03msymbolic links encountered in the path.\"\"\"\u001b[39;00m\n\u001b[1;32m    395\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(filename)\n\u001b[0;32m--> 396\u001b[0m     path, ok \u001b[38;5;241m=\u001b[39m \u001b[43m_joinrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m abspath(path)\n",
      "File \u001b[0;32m~/orcd/pool/.conda/envs/LLM/lib/python3.10/posixpath.py:431\u001b[0m, in \u001b[0;36m_joinrealpath\u001b[0;34m(path, rest, strict, seen)\u001b[0m\n\u001b[1;32m    429\u001b[0m newpath \u001b[38;5;241m=\u001b[39m join(path, name)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 431\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if total_threads == 1: ##merging mode: This block only runs if you're not distributing work across multiple threads.\n",
    "\n",
    "  \n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    # from tqdm.notebook import tqdm\n",
    "    # from IPython.display import display, Markdown\n",
    "    \n",
    "    \n",
    "    # tokenizer_model=f'/home/mkychsu/pool/llm/SEMIKONG-8b-GPTQ'\n",
    "    # tokenizer_model=f'/home/mkychsu/pool/llm/nomic-embed-text-v1.5'\n",
    "    \n",
    "    # embedding_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model,use_fast=False, device_map=\"cuda:0\")\n",
    "    # embedding_model = AutoModelForCausalLM.from_pretrained(tokenizer_model,output_hidden_states=True).to('cuda:0')\n",
    "    \n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    embedding_tokenizer =''\n",
    "    embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "    \n",
    "    from GraphReasoning import load_embeddings, save_embeddings, generate_hypernode_embeddings\n",
    "    \n",
    "    # generate_new_embeddings=True\n",
    "    \n",
    "    # from PIL import Image\n",
    "    # from transformers import AutoModelForCausalLM \n",
    "    # from transformers import AutoProcessor \n",
    "    \n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda:1\", trust_remote_code=True, torch_dtype=\"auto\")\n",
    "    # processor = AutoProcessor.from_pretrained(model_id, device_map=\"cuda:1\", trust_remote_code=True) \n",
    "    import hypernetx as hnx\n",
    "\n",
    "    import torch\n",
    "    generate_new_embeddings=True\n",
    "\n",
    "#This block checks if the embedding file already existsâ€¦ but then immediately overrides the flag to True anyway, which forces re-generation.\n",
    "    if os.path.exists(f'{data_dir}/{embedding_file}'):\n",
    "        print('Found existing embedding file')\n",
    "        generate_new_embeddings=False\n",
    "    generate_new_embeddings=True\n",
    "\n",
    "    #iterates over graph nodes and embeds them using the model \n",
    "    # with torch.no_grad():\n",
    "    #     if generate_new_embeddings:\n",
    "    #         G=nx.DiGraph()\n",
    "    #         node_embeddings = generate_node_embeddings(G, embedding_tokenizer, embedding_model, )\n",
    "    #         save_embeddings(node_embeddings, f'{data_dir}/{embedding_file}')\n",
    "    #     else:\n",
    "    #         node_embeddings = load_embeddings(f'{data_dir}/{embedding_file}')\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        if generate_new_embeddings:\n",
    "            H = hnx.Hypergraph({})\n",
    "            # Extract node IDs (will be empty here)\n",
    "            nodes = list(H.nodes)\n",
    "            # Generate embeddings for (new) nodes\n",
    "            node_embeddings = generate_hypernode_embeddings(\n",
    "                nodes,\n",
    "                embedding_tokenizer,\n",
    "                embedding_model,\n",
    "            )\n",
    "            # Save them\n",
    "            save_embeddings(node_embeddings, f'{data_dir}/{embedding_file}')\n",
    "        else:\n",
    "            # Load previously computed embeddings\n",
    "            node_embeddings = load_embeddings(f'{data_dir}/{embedding_file}')                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e892a7b-ec8c-4385-b429-d3c10c49ec52",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e47a90-fe76-4c50-8468-64dbcfaf76f7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Load dataset of papers\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "### Load all markdown files (dataset of papers) --- assumes each subfolder in doc_data_dir is named like a paper ID\n",
    "#doc_list becomes a list of full file paths to .md files\n",
    "\n",
    "doc_list=[]\n",
    "with os.scandir(f'{doc_data_dir}') as folders:\n",
    "    for folder in folders:\n",
    "        doc_list.append(f'{doc_data_dir}/{folder.name}/{folder.name}.md')\n",
    "\n",
    "doc_list=sorted(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b927a8-b449-4297-a6e0-9fd82b08ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_data_dir = './missingfiles' #place where you keep your markdown files\n",
    "\n",
    "# #temporary fix to the missing md files \n",
    "# doc_list = []\n",
    "# with os.scandir(doc_data_dir) as entries:\n",
    "#     for entry in entries:\n",
    "#         if entry.is_file() and entry.name.endswith('.md'):\n",
    "#             doc_list.append(os.path.join(doc_data_dir, entry.name))\n",
    "\n",
    "# doc_list = sorted(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393e1660",
   "metadata": {},
   "source": [
    "### Set up LLM client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f616e017-9eed-4354-8763-8f60221d5a0d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import instructor\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "import base64\n",
    "from pydantic import BaseModel\n",
    "from instructor import patch\n",
    "\n",
    "\n",
    "#new for hypergraph\n",
    "class Event(BaseModel):\n",
    "    source: List[str]\n",
    "    # target: str\n",
    "    target: List[str]\n",
    "    relation: str \n",
    "\n",
    "class HypergraphJSON(BaseModel):\n",
    "    events: List[Event]\n",
    "\n",
    "\n",
    "#Identifies phrases or terms that are potential nodes --> Decide the type of each node --> Determine which nodes are related, and what the relation is \n",
    "\n",
    "response_model = HypergraphJSON\n",
    "system_prompt = '''\n",
    "                 (\n",
    "                    \"You are a network ontology graph maker who extracts terms and their relations from a given context, using principles from category theory.\\n\\n\"\n",
    "\n",
    "                    \"You are provided with a context chunk (delimited by triple backticks: ```). Your task is to extract an ontology of terms mentioned in the context, representing key scientific concepts, systems, materials, and methods using well-defined, technical, and widely accepted terminology.\\n\\n\"\n",
    "\n",
    "                    \"Proceed step by step:\\n\"\n",
    "                    \"Thought 1: Traverse the text sentence by sentence. Identify key scientific terms, such as materials, methods, entities, systems, conditions, or acronyms. \\n\"\n",
    "                    \"    - Focus on extracting terms that are atomistic and domain-relevant.\\n\"\n",
    "                    \"    - Group modifiers (e.g., 'collagen scaffold') as one term if they form a recognized concept.\\n\\n\"\n",
    "\n",
    "                    \"Thought 2: Determine which terms are related to each other based on their co-occurrence in a sentence or paragraph.\\n\"\n",
    "                    \"    - A term may relate to multiple other terms.\\n\"\n",
    "                    \"    - Look for structural, functional, or procedural relationships.\\n\\n\"\n",
    "\n",
    "                    \"Thought 3: For each related group of terms, infer the scientific relationship between them.\\n\"\n",
    "                    \"    - Use category-theoretic relation names when possible, such as: 'is', 'has', 'acts on', 'used for', 'composed of', 'leads to'.\\n\"\n",
    "                    \"    - If 3 or more co-dependent entities relate to a shared target, use an n-ary relation with the source as a list.\\n\"\n",
    "                    \"    - If only 2 entities are involved, use a binary relation.\\n\\n\"\n",
    "\n",
    "                    \"Output Specification:\\n\"\n",
    "                    \"Return a JSON object with a single field: 'events'. Each event must contain:\\n\"\n",
    "                    \"- 'source': a string (for binary) or a list of entities (for n-ary)\\n\"\n",
    "                    \"- 'target': the main concept or object being acted upon or described\\n\"\n",
    "                    \"- 'relation': a concise, meaningful phrase describing the relation between source and target\\n\\n\"\n",
    "\n",
    "                    \"Important:\\n\"\n",
    "                    \"- Always preserve the original wording for technical terms.\\n\"\n",
    "                    \"- Do not invent entities or relations that are not implied in the text.\\n\"\n",
    "                    \"- Do not include any additional fields beyond 'source', 'target', and 'relation'.\\n\\n\"\n",
    "\n",
    "                    \"Examples:\\n\\n\"\n",
    "\n",
    "                    \"Binary relation:\\n\"\n",
    "                    \"{\\n\"\n",
    "                    \"  \\\"source\\\": \\\"hydrangea\\\",\\n\"\n",
    "                    \"  \\\"target\\\": \\\"flower\\\",\\n\"\n",
    "                    \"  \\\"relation\\\": \\\"is a type of\\\"\\n\"\n",
    "                    \"}\\n\\n\"\n",
    "\n",
    "                    \"N-ary relation:\\n\"\n",
    "                    \"{\\n\"\n",
    "                    \"  \\\"source\\\": [\\\"Sally\\\", \\\"Bob\\\", \\\"Julia\\\"],\\n\"\n",
    "                    \"  \\\"target\\\": \\\"paper 1\\\",\\n\"\n",
    "                    \"  \\\"relation\\\": \\\"are equal co-authors of\\\"\\n\"\n",
    "                    \"}\\n\\n\"\n",
    "\n",
    "                    \"Return a JSON object with this structure:\\n\"\n",
    "                    \"{\\n\"\n",
    "                    \"  \\\"events\\\": [\\n\"\n",
    "                    \"     {\\\"source\\\": ..., \\\"target\\\": ..., \\\"relation\\\": ...},\\n\"\n",
    "                    \"     {...},\\n\"\n",
    "                    \"     ...\\n\"\n",
    "                    \"  ]\\n\"\n",
    "                    \"}\"\n",
    "                    )\n",
    "'''\n",
    "\n",
    "def generate(system_prompt=system_prompt, \n",
    "             prompt=\"\",temperature=0.333,\n",
    "             max_tokens=config_list[0]['max_tokens'], response_model=HypergraphJSON, \n",
    "            ):     \n",
    "\n",
    "    if system_prompt==None:\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
    "        ]\n",
    "\n",
    "    else:\n",
    "        messages=[\n",
    "            {\"role\": \"system\",  \"content\": f\"{system_prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
    "        ]\n",
    "\n",
    "    # monkey patching: replacing or enhancing client.chat.completions.create\n",
    "    create = instructor.patch(     #instructor is a python library that wraps LLM responses and validates LLM output against a schema you define (via pydantic)\n",
    "        create=client.chat.completions.create,    #Automatically converts the response into a real Python object (not just a raw string or dictionary)\n",
    "        mode=instructor.Mode.JSON_SCHEMA,\n",
    "    ) \n",
    "    \n",
    "\n",
    "    return create(messages=messages,   \n",
    "                    model=config_list[0][\"model\"],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=0.333,\n",
    "                    response_model=response_model,\n",
    "                   )\n",
    "\n",
    "def image_to_base64_data_uri(file_path):\n",
    "    with open(file_path, \"rb\") as image_file:\n",
    "        base64_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        return f\"data:image/png;base64,{base64_data}\"\n",
    "\n",
    "def generate_figure(image, system_prompt=system_prompt, \n",
    "                prompt=\"\", temperature=0,\n",
    "                ):\n",
    "\n",
    "    pwd = os.getcwd()\n",
    "    image = image.split(pwd)[-1]\n",
    "    image=Path('.').glob(f'**/{image}', case_sensitive=False)\n",
    "    image = list(image)[0]\n",
    "\n",
    "    image_uri = image_to_base64_data_uri(image)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_uri}},\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image in detail please.\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "        \n",
    "    return create(messages=messages,   \n",
    "                    model=config_list[0][\"model\"],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=0.333,\n",
    "                    response_model=response_model,\n",
    "                   ).choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5cd7e7-b31d-4330-b374-762655a2f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'running on {thread_i}-th thread in totally {total_threads} threads') #double check threading "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a2225-cb47-47e8-9c6c-89e1d94b99eb",
   "metadata": {},
   "source": [
    "### Checkpoint Cell: Finds where merging left off with current_merged_i. Handles resuming graph merging in single-threaded mode\n",
    "\n",
    "#### 1. Finds the latest merged graph from previous runs.\n",
    "\n",
    "#### 2. Checks if it's valid by trying to load it.\n",
    "\n",
    "#### 3. If the graph or its parts are corrupted, it deletes them and rolls back to the previous merge index.\n",
    "\n",
    "#### 4. Sets current_merged_i so merging can resume cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d538e00-b605-4313-95d3-780c6c97369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# if total_threads == 1: # merging mode\n",
    "#     # try:\n",
    "#     merged_graph_list = sorted(glob.glob(f'{data_dir_output}/*_integrated.graphml'), reverse=True, key = lambda x: int(x.split('_')[2].split('/')[-1]))\n",
    "#     current_merged_i = int(merged_graph_list[0].split('_')[2].split('/')[-1])\n",
    "#     last_graph = sorted(glob.glob(f'{data_dir}/{current_merged_i}_*.graphml') )\n",
    "    \n",
    "#     # trim the working graphs from the last session if it is be broken.\n",
    "#     try: \n",
    "#         nx.read_graphml(merged_graph_list[0])\n",
    "#         nx.read_graphml(last_graph)\n",
    "#     except:\n",
    "#         if os.path.exists(merged_graph_list[0]):\n",
    "#             os.remove(merged_graph_list[0])\n",
    "#         last_graph_files = sorted(glob.glob(f'{data_dir}/{current_merged_i}_*'), reverse=True)\n",
    "  \n",
    "#         for file in last_graph_files:\n",
    "#             if os.path.exists(file):\n",
    "#                 os.remove(file)\n",
    "#         current_merged_i -= 1\n",
    "\n",
    "#     print(f'Start merging from No. {current_merged_i}')\n",
    "# else:\n",
    "#     current_merged_i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba78f9c-db8f-4ba4-a8ac-9d929af7d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next part which is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc592b15-7023-42b5-bad8-b82f0a2e2534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, re, glob, pickle\n",
    "\n",
    "# INT_PREFIX = re.compile(r'^(\\d+)_')\n",
    "\n",
    "# def extract_idx(path: str) -> int:\n",
    "#     \"\"\"Return leading integer prefix before first underscore, else -1.\"\"\"\n",
    "#     name = os.path.basename(path)\n",
    "#     m = INT_PREFIX.match(name)\n",
    "#     return int(m.group(1)) if m else -1\n",
    "\n",
    "# if total_threads == 1:  # merging mode\n",
    "#     # 1) find all â€œ*_integrated.pklâ€ files\n",
    "#     merged_graph_list = sorted(\n",
    "#         glob.glob(f'{data_dir_output}/*_integrated.pkl'),\n",
    "#         reverse=True,\n",
    "#         key=extract_idx\n",
    "#     )\n",
    "\n",
    "#     # 2) determine current merged index; fall back to 0 if none found\n",
    "#     current_merged_i = extract_idx(merged_graph_list[0]) if merged_graph_list else 0\n",
    "\n",
    "#     # 3) find any per-session files for the current index\n",
    "#     last_graph = sorted(\n",
    "#         glob.glob(f'{data_dir}/{current_merged_i}_*.pkl'),\n",
    "#         reverse=True\n",
    "#     )\n",
    "\n",
    "#     # 4) validate heads if both lists non-empty\n",
    "#     if merged_graph_list and last_graph:\n",
    "#         try:\n",
    "#             with open(merged_graph_list[0], 'rb') as f:\n",
    "#                 _ = pickle.load(f)\n",
    "#             with open(last_graph[0], 'rb') as f:\n",
    "#                 _ = pickle.load(f)\n",
    "#         except Exception:\n",
    "#             print(\"Validation failed; cleaning up corrupt files.\")\n",
    "#             try:\n",
    "#                 os.remove(merged_graph_list[0])\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "#             for fn in last_graph:\n",
    "#                 try:\n",
    "#                     os.remove(fn)\n",
    "#                 except Exception:\n",
    "#                     pass\n",
    "#             current_merged_i = max(0, current_merged_i - 1)\n",
    "#     else:\n",
    "#         if not merged_graph_list:\n",
    "#             print(\"No integrated pickles; starting at 0.\")\n",
    "#         else:\n",
    "#             print(\"No per-session pickles to validate; skipping load check.\")\n",
    "\n",
    "#     print(f\"Start merging from No. {current_merged_i}\")\n",
    "\n",
    "# else:\n",
    "#     current_merged_i = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113362c0-185b-4ae9-be5f-5be1317c7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, pickle\n",
    "\n",
    "INT_PREFIX = re.compile(r'^(\\d+)_')\n",
    "\n",
    "def extract_idx(path: str) -> int:\n",
    "    \"\"\"Return leading integer prefix before first underscore, else -1.\"\"\"\n",
    "    name = os.path.basename(path)\n",
    "    m = INT_PREFIX.match(name)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "# Directories\n",
    "doc_data_dir = './CompositePDFs_marker'\n",
    "data_dir = './GRAPHDATA_paper'\n",
    "data_dir_output = './GRAPHDATA_OUTPUT_paper'\n",
    "\n",
    "if total_threads == 1:  # merging mode\n",
    "    # 1) find all \"*_integrated.pkl\" files\n",
    "    merged_graph_list = sorted(\n",
    "        glob.glob(f'{data_dir_output}/*_integrated.pkl'),\n",
    "        reverse=True,\n",
    "        key=extract_idx\n",
    "    )\n",
    "\n",
    "    # 2) determine current merged index; fall back to 0 if none found\n",
    "    current_merged_i = extract_idx(merged_graph_list[0]) if merged_graph_list else 0\n",
    "\n",
    "    # 3) find any per-session files for the current index\n",
    "    last_graph = sorted(\n",
    "        glob.glob(f'{data_dir}/{current_merged_i}_*.pkl'),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    # 4) validate heads if both lists non-empty\n",
    "    if merged_graph_list and last_graph:\n",
    "        try:\n",
    "            with open(merged_graph_list[0], 'rb') as f:\n",
    "                _ = pickle.load(f)\n",
    "            with open(last_graph[0], 'rb') as f:\n",
    "                _ = pickle.load(f)\n",
    "        except Exception:\n",
    "            print(\"Validation failed; cleaning up corrupt files.\")\n",
    "            try:\n",
    "                os.remove(merged_graph_list[0])\n",
    "            except Exception:\n",
    "                pass\n",
    "            for fn in last_graph:\n",
    "                try:\n",
    "                    os.remove(fn)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            current_merged_i = max(0, current_merged_i - 1)\n",
    "    else:\n",
    "        if not merged_graph_list:\n",
    "            print(\"No integrated pickles; starting at 0.\")\n",
    "        else:\n",
    "            print(\"No per-session pickles to validate; skipping load check.\")\n",
    "\n",
    "    print(f\"Start merging from No. {current_merged_i}\")\n",
    "\n",
    "    # === Filter doc_list to remove missing markdown files ===\n",
    "    missing_log = os.path.join(data_dir_output, \"missing_markdown_folders.txt\")\n",
    "    \n",
    "    # clear old log once at the start of run\n",
    "    open(missing_log, \"w\").close()\n",
    "    \n",
    "    valid_doc_list = []\n",
    "    \n",
    "    for doc_path in doc_list:\n",
    "        # Check if the markdown file actually exists\n",
    "        if not os.path.exists(doc_path):\n",
    "            folder_path = os.path.dirname(doc_path)\n",
    "            print(f\"âš ï¸ Missing markdown for: {folder_path}, skipping.\")\n",
    "            with open(missing_log, \"a\") as logf:\n",
    "                logf.write(folder_path + \"\\n\")\n",
    "            continue\n",
    "        \n",
    "        valid_doc_list.append(doc_path)\n",
    "    \n",
    "    # Replace doc_list with filtered version\n",
    "    doc_list = valid_doc_list\n",
    "    print(f\"âœ“ Filtered doc_list: {len(doc_list)} valid documents (excluded missing markdown files)\")\n",
    "\n",
    "else:\n",
    "    current_merged_i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bfb33b-771c-46f2-acd7-26ea3f25e0a6",
   "metadata": {},
   "source": [
    "### Generate a Knowledge Graph (KG) from each document using an LLM + embedding pipeline, then (optionally) merge it into a global graph in merging mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56449435-0d69-49c6-a148-018a7bc12b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pickle\n",
    "# import hypernetx as hnx\n",
    "# from GraphReasoning import make_hypergraph_from_text, add_new_hypersubgraph_from_text, update_hypernode_embeddings\n",
    "# from datetime import datetime\n",
    "# import time\n",
    "# import torch\n",
    "# import shutil\n",
    "# import traceback\n",
    "\n",
    "\n",
    "# # Initialize the â€œglobalâ€ graph\n",
    "# G = hnx.Hypergraph({})\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i, doc in enumerate(doc_list):\n",
    "#         # only process docs for this thread\n",
    "#         if i % total_threads != thread_i:\n",
    "#             continue\n",
    "#         # skip already-merged docs\n",
    "#         if i < current_merged_i:\n",
    "#             continue\n",
    "\n",
    "#         # extract title/doi and text\n",
    "#         title = os.path.basename(doc).rsplit('.md', 1)[0]\n",
    "#         doi = title\n",
    "#         with open(doc, 'r') as f:\n",
    "#             txt = f.read()\n",
    "\n",
    "#         # define where this docâ€™s subgraph lives\n",
    "#         graph_root = f'{i}_{title[:100]}'\n",
    "#         current_graph = os.path.join(data_dir, f'{graph_root}.pkl')\n",
    "\n",
    "#         # generate until the subgraph file appears\n",
    "#         while not os.path.exists(current_graph):\n",
    "#             print(f\"Generating KG for {i}: {title}\")\n",
    "#             try:\n",
    "#                 if not isinstance(txt, str):\n",
    "#                     print(\"Text is not a string:\", txt)\n",
    "#                     break\n",
    "#                 now = datetime.now()\n",
    "#                 current_graph, _, _, _,  = make_hypergraph_from_text(\n",
    "#                     txt, generate, generate_figure, image_list='',\n",
    "#                     graph_root=graph_root,\n",
    "#                     do_distill=False,\n",
    "#                     do_relabel=False,\n",
    "#                     chunk_size=10000, chunk_overlap=0,\n",
    "#                     repeat_refine=0, verbatim=False,\n",
    "#                     data_dir=data_dir,\n",
    "#                 )\n",
    "#                 print(\"Time:\", datetime.now() - now)\n",
    "#             except Exception as e:\n",
    "#                 print(\"Error during KG generation:\", repr(e))\n",
    "#                 time.sleep(60)\n",
    "\n",
    "#         # â”€â”€ MERGING MODE â”€â”€\n",
    "#         if total_threads == 1:\n",
    "\n",
    "#             if i % merge_every == 0:\n",
    "#                 do_simplify_graph = True\n",
    "#                 size_threshold = 10\n",
    "#             else:\n",
    "#                 do_simplify_graph = False\n",
    "#                 size_threshold = 0\n",
    "                \n",
    "#             _hypergraph_pkl = os.path.join(data_dir_output, f'{graph_root}_integrated.pkl')\n",
    "#             # skip if already merged\n",
    "#             if os.path.exists(_hypergraph_pkl):\n",
    "#                 with open(_hypergraph_pkl, 'rb') as f:\n",
    "#                     G = pickle.load(f)  \n",
    "#                 print(f\"[merge] already have integrated at {_hypergraph_pkl}\")\n",
    "#                 continue            \n",
    "            \n",
    "#             now = datetime.now()\n",
    "#             print(f\"[merge] about to merge paper {i}: will write {_hypergraph_pkl!r}\")\n",
    "\n",
    "#             try:\n",
    "#                 graph_path = current_graph  # save the path\n",
    "#                 with open(graph_path, 'rb') as f:\n",
    "#                     H0 = pickle.load(f)\n",
    "#                 current_graph = hnx.Hypergraph(\n",
    "#                     H0.incidence_dict,\n",
    "#                     edge_attr={'DOI': {eid: doi for eid in H0.incidence_dict}}\n",
    "#                 )\n",
    "#                 print(f\"[merge] loaded subgraph with {len(current_graph.edges)} edges\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"[merge] failed loading/annotating {graph_path!r}: {e!r}\")\n",
    "#                 continue\n",
    "\n",
    "#             #OLD CODE\n",
    "#             # fullpath = os.path.join(data_dir, \"original_sub_dfs.pkl\")\n",
    "#             # with open(fullpath, \"rb\") as f:\n",
    "#             #     sub_dfs = pickle.load(f)\n",
    "\n",
    "#             ### FIX: Load sub_dfs cumulatively\n",
    "#             updated_path = os.path.join(data_dir_output, \"updated_sub_dfs.pkl\")\n",
    "#             if os.path.exists(updated_path):\n",
    "#                 with open(updated_path, \"rb\") as f:\n",
    "#                     sub_dfs = pickle.load(f)\n",
    "#                 print(f\"[merge] Loaded cumulative sub_dfs from {updated_path}\")\n",
    "#             else:\n",
    "#                 fullpath = os.path.join(data_dir, \"original_sub_dfs.pkl\")\n",
    "#                 with open(fullpath, \"rb\") as f:\n",
    "#                     sub_dfs = pickle.load(f)\n",
    "#                 print(f\"[merge] Loaded initial sub_dfs from {fullpath}\")\n",
    "\n",
    "            \n",
    "#             # perform the merge\n",
    "#             integrated_path, G, _, node_embeddings, sub_dfs = add_new_hypersubgraph_from_text(\n",
    "#                 txt='',\n",
    "#                 node_embeddings=node_embeddings,\n",
    "#                 tokenizer=embedding_tokenizer,\n",
    "#                 model=embedding_model,\n",
    "#                 original_graph=G,\n",
    "#                 data_dir_output=data_dir_output,\n",
    "#                 graph_root=graph_root,\n",
    "#                 do_simplify_graph=do_simplify_graph,\n",
    "#                 do_relabel=False,\n",
    "#                 size_threshold=size_threshold,\n",
    "#                 do_update_node_embeddings=do_simplify_graph,\n",
    "#                 repeat_refine=0,\n",
    "#                 similarity_threshold=0.9,\n",
    "#                 do_Louvain_on_new_graph=False,\n",
    "#                 return_only_giant_component=False,\n",
    "#                 save_common_graph=False,\n",
    "#                 G_to_add=current_graph, #G_to_add=H_sub, \n",
    "#                 graph_pkl_to_add=None, \n",
    "#                 sub_dfs=sub_dfs,\n",
    "#                 verbatim=True,\n",
    "#             )\n",
    "\n",
    "#             # â”€â”€ CONDITIONAL EMBEDDING UPDATE â”€â”€\n",
    "#             doc_count = len(doc_list)\n",
    "#             is_last_group = (doc_count < 100) or (i >= (doc_count // 100) * 100)\n",
    "\n",
    "#             if is_last_group:\n",
    "#                 try:\n",
    "#                     print(f\"[update] Performing embedding update for doc {i} (triggered by small or remainder group)\")\n",
    "#                     node_embeddings = update_hypernode_embeddings(node_embeddings, G, embedding_tokenizer, embedding_model)\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"[update] Failed to update embeddings: {e!r}\")\n",
    "\n",
    "#             # â”€â”€ check consistency â”€â”€\n",
    "#             graph_nodes = set(str(n) for n in G.nodes)\n",
    "#             embedding_keys = set(str(k) for k in node_embeddings)\n",
    "        \n",
    "#             missing = graph_nodes - embedding_keys\n",
    "#             extra = embedding_keys - graph_nodes\n",
    "        \n",
    "#             if not missing and not extra:\n",
    "#                 print(f\"[check] Embeddings are aligned with the graph nodes. Count = {len(graph_nodes)}\")\n",
    "#             else:\n",
    "#                 print(f\"[check] Embedding mismatch detected:\")\n",
    "#                 if missing:\n",
    "#                     print(f\"  - Missing embeddings for {len(missing)} nodes: {list(missing)[:5]}\")\n",
    "#                 if extra:\n",
    "#                     print(f\"  - Extra embeddings for {len(extra)} nodes not in graph: {list(extra)[:5]}\")\n",
    "                        \n",
    "#             #rename the very final last saved clean graph \n",
    "#             is_last_doc = (i == len(doc_list) - 1)\n",
    "                        \n",
    "#             if is_last_doc:\n",
    "#                 final_path = os.path.join(data_dir_output, \"final_graph.pkl\")\n",
    "#                 try:\n",
    "#                     shutil.copyfile(integrated_path, final_path)\n",
    "#                     print(f\"[merge] Final graph saved as: {final_path}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"[merge] Failed to rename final graph: {e!r}\")\n",
    "            \n",
    "#             try:\n",
    "#                 save_embeddings(node_embeddings, os.path.join(data_dir, embedding_file))\n",
    "#                 print(\"[merge] embeddings saved\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"[merge] failed to save embeddings: {e!r}\")\n",
    "            \n",
    "#             print(\"Merge elapsed time:\", datetime.now() - now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53a91a-c933-48a8-b958-4241edeb4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import hypernetx as hnx\n",
    "from GraphReasoning import make_hypergraph_from_text, add_new_hypersubgraph_from_text, update_hypernode_embeddings\n",
    "from datetime import datetime\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import traceback\n",
    "\n",
    "\n",
    "# Initialize the \"global\" graph\n",
    "G = hnx.Hypergraph({})\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, doc in enumerate(doc_list):\n",
    "        # only process docs for this thread\n",
    "        if i % total_threads != thread_i:\n",
    "            continue\n",
    "        # skip already-merged docs\n",
    "        if i < current_merged_i:\n",
    "            continue\n",
    "\n",
    "        # extract title/doi and text\n",
    "        title = os.path.basename(doc).rsplit('.md', 1)[0]\n",
    "        doi = title\n",
    "        with open(doc, 'r') as f:\n",
    "            txt = f.read()\n",
    "\n",
    "        # define where this doc's subgraph lives\n",
    "        graph_root = f'{i}_{title[:100]}'\n",
    "        current_graph = os.path.join(data_dir, f'{graph_root}.pkl')\n",
    "\n",
    "        # Variable to store current document's sub_dfs\n",
    "        current_doc_sub_dfs = None\n",
    "        \n",
    "        # generate until the subgraph file appears\n",
    "        while not os.path.exists(current_graph):\n",
    "            print(f\"Generating KG for {i}: {title}\")\n",
    "            try:\n",
    "                if not isinstance(txt, str):\n",
    "                    print(\"Text is not a string:\", txt)\n",
    "                    break\n",
    "                now = datetime.now()\n",
    "                # FIX: Capture all return values including sub_dfs\n",
    "                current_graph, _, sub_dfs_pkl_path, current_doc_sub_dfs = make_hypergraph_from_text(\n",
    "                    txt, generate, generate_figure, image_list='',\n",
    "                    graph_root=graph_root,\n",
    "                    do_distill=False,\n",
    "                    do_relabel=False,\n",
    "                    chunk_size=10000, chunk_overlap=0,\n",
    "                    repeat_refine=0, verbatim=False,\n",
    "                    data_dir=data_dir,\n",
    "                )\n",
    "                print(\"Time:\", datetime.now() - now)\n",
    "                print(f\"[generation] Captured sub_dfs with {len(current_doc_sub_dfs) if current_doc_sub_dfs else 0} chunks\")\n",
    "            except Exception as e:\n",
    "                print(\"Error during KG generation:\", repr(e))\n",
    "                time.sleep(60)\n",
    "\n",
    "        # â”€â”€ MERGING MODE â”€â”€\n",
    "        if total_threads == 1:\n",
    "\n",
    "            if i % merge_every == 0:\n",
    "                do_simplify_graph = True\n",
    "                size_threshold = 10\n",
    "            else:\n",
    "                do_simplify_graph = False\n",
    "                size_threshold = 0\n",
    "                \n",
    "            _hypergraph_pkl = os.path.join(data_dir_output, f'{graph_root}_integrated.pkl')\n",
    "            # skip if already merged\n",
    "            if os.path.exists(_hypergraph_pkl):\n",
    "                with open(_hypergraph_pkl, 'rb') as f:\n",
    "                    G = pickle.load(f)  \n",
    "                print(f\"[merge] already have integrated at {_hypergraph_pkl}\")\n",
    "                continue            \n",
    "            \n",
    "            now = datetime.now()\n",
    "            print(f\"[merge] about to merge paper {i}: will write {_hypergraph_pkl!r}\")\n",
    "\n",
    "            try:\n",
    "                graph_path = current_graph  # save the path\n",
    "                with open(graph_path, 'rb') as f:\n",
    "                    H0 = pickle.load(f)\n",
    "                current_graph = hnx.Hypergraph(\n",
    "                    H0.incidence_dict,\n",
    "                    edge_attr={'DOI': {eid: doi for eid in H0.incidence_dict}}\n",
    "                )\n",
    "                print(f\"[merge] loaded subgraph with {len(current_graph.edges)} edges\")\n",
    "            except Exception as e:\n",
    "                print(f\"[merge] failed loading/annotating {graph_path!r}: {e!r}\")\n",
    "                continue\n",
    "\n",
    "            ### FIX: Load cumulative sub_dfs\n",
    "            updated_path = os.path.join(data_dir_output, \"updated_sub_dfs.pkl\")\n",
    "            if os.path.exists(updated_path):\n",
    "                with open(updated_path, \"rb\") as f:\n",
    "                    sub_dfs = pickle.load(f)\n",
    "                print(f\"[merge] Loaded cumulative sub_dfs from {updated_path} with {len(sub_dfs)} existing chunks\")\n",
    "            else:\n",
    "                sub_dfs = []\n",
    "                print(f\"[merge] Starting fresh sub_dfs list\")\n",
    "\n",
    "            ### FIX: If generation happened (and wasn't skipped), add current doc's chunks\n",
    "            if current_doc_sub_dfs is None:\n",
    "                # Generation was skipped (file already existed), load from the saved pkl\n",
    "                sub_dfs_pkl_path = os.path.join(data_dir, \"original_sub_dfs.pkl\")\n",
    "                if os.path.exists(sub_dfs_pkl_path):\n",
    "                    with open(sub_dfs_pkl_path, \"rb\") as f:\n",
    "                        current_doc_sub_dfs = pickle.load(f)\n",
    "                    print(f\"[merge] Loaded current doc's sub_dfs from {sub_dfs_pkl_path}\")\n",
    "            \n",
    "            ### FIX: Append current document's chunks to cumulative list\n",
    "            if current_doc_sub_dfs:\n",
    "                if isinstance(current_doc_sub_dfs, list):\n",
    "                    sub_dfs.extend(current_doc_sub_dfs)\n",
    "                    print(f\"[merge] Added {len(current_doc_sub_dfs)} chunks from doc {i}. Total: {len(sub_dfs)} chunks\")\n",
    "                else:\n",
    "                    sub_dfs.append(current_doc_sub_dfs)\n",
    "                    print(f\"[merge] Added 1 chunk from doc {i}. Total: {len(sub_dfs)} chunks\")\n",
    "            else:\n",
    "                print(f\"[merge] WARNING: No sub_dfs found for doc {i}\")\n",
    "\n",
    "            \n",
    "            # perform the merge\n",
    "            integrated_path, G, _, node_embeddings, sub_dfs = add_new_hypersubgraph_from_text(\n",
    "                txt='',\n",
    "                node_embeddings=node_embeddings,\n",
    "                tokenizer=embedding_tokenizer,\n",
    "                model=embedding_model,\n",
    "                original_graph=G,\n",
    "                data_dir_output=data_dir_output,\n",
    "                graph_root=graph_root,\n",
    "                do_simplify_graph=do_simplify_graph,\n",
    "                do_relabel=False,\n",
    "                size_threshold=size_threshold,\n",
    "                do_update_node_embeddings=do_simplify_graph,\n",
    "                repeat_refine=0,\n",
    "                similarity_threshold=0.9,\n",
    "                do_Louvain_on_new_graph=False,\n",
    "                return_only_giant_component=False,\n",
    "                save_common_graph=False,\n",
    "                G_to_add=current_graph,\n",
    "                graph_pkl_to_add=None, \n",
    "                sub_dfs=sub_dfs,  # Now contains all previous + current doc's chunks\n",
    "                verbatim=True,\n",
    "            )\n",
    "\n",
    "            print(f\"[merge] After merge, sub_dfs contains {len(sub_dfs)} total chunks\")\n",
    "\n",
    "            # â”€â”€ CONDITIONAL EMBEDDING UPDATE â”€â”€\n",
    "            doc_count = len(doc_list)\n",
    "            is_last_group = (doc_count < 100) or (i >= (doc_count // 100) * 100)\n",
    "\n",
    "            if is_last_group:\n",
    "                try:\n",
    "                    print(f\"[update] Performing embedding update for doc {i} (triggered by small or remainder group)\")\n",
    "                    node_embeddings = update_hypernode_embeddings(node_embeddings, G, embedding_tokenizer, embedding_model)\n",
    "                except Exception as e:\n",
    "                    print(f\"[update] Failed to update embeddings: {e!r}\")\n",
    "\n",
    "            # â”€â”€ check consistency â”€â”€\n",
    "            graph_nodes = set(str(n) for n in G.nodes)\n",
    "            embedding_keys = set(str(k) for k in node_embeddings)\n",
    "        \n",
    "            missing = graph_nodes - embedding_keys\n",
    "            extra = embedding_keys - graph_nodes\n",
    "        \n",
    "            if not missing and not extra:\n",
    "                print(f\"[check] Embeddings are aligned with the graph nodes. Count = {len(graph_nodes)}\")\n",
    "            else:\n",
    "                print(f\"[check] Embedding mismatch detected:\")\n",
    "                if missing:\n",
    "                    print(f\"  - Missing embeddings for {len(missing)} nodes: {list(missing)[:5]}\")\n",
    "                if extra:\n",
    "                    print(f\"  - Extra embeddings for {len(extra)} nodes not in graph: {list(extra)[:5]}\")\n",
    "                        \n",
    "            #rename the very final last saved clean graph \n",
    "            is_last_doc = (i == len(doc_list) - 1)\n",
    "                        \n",
    "            if is_last_doc:\n",
    "                final_path = os.path.join(data_dir_output, \"final_graph.pkl\")\n",
    "                try:\n",
    "                    shutil.copyfile(integrated_path, final_path)\n",
    "                    print(f\"[merge] Final graph saved as: {final_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[merge] Failed to rename final graph: {e!r}\")\n",
    "            \n",
    "            try:\n",
    "                save_embeddings(node_embeddings, os.path.join(data_dir, embedding_file))\n",
    "                print(\"[merge] embeddings saved\")\n",
    "            except Exception as e:\n",
    "                print(f\"[merge] failed to save embeddings: {e!r}\")\n",
    "            \n",
    "            print(\"Merge elapsed time:\", datetime.now() - now)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
