{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1a4c4a0",
   "metadata": {},
   "source": [
    "# Using LLMs and Knowledge graphs to search for PFAS Alternatives\n",
    "\n",
    "## Project with Saint Gobain\n",
    "\n",
    "#### Yu-Chuan (Michael) Hsu, Isabella Stewart, Tarjei Hage, Wei Lu, and Markus J. Buehler, MIT, 2025 \n",
    "mkychsu@MIT.EDU, istewart@MIT.EDU, tphage@MIT.EDU, wl7@MIT.EDU, mbuehler@MIT.EDU\n",
    "#### LAMM, Massachusetts Institute of Technology\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c2660-bd21-4c34-a4b4-748f469d8925",
   "metadata": {},
   "source": [
    "# Allows for distributed or parallel processing of a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ebbae2c-62eb-4878-a45f-2f305b16bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "try:\n",
    "    thread_i = int(sys.argv[1]) #which thread number this process is (e.g., in multi-threaded runs)\n",
    "    total_threads = int(sys.argv[2]) #how many total threads are running\n",
    "\n",
    "except: \n",
    "    thread_i = 0 #If no arguments are provided (e.g. during a notebook run), it defaults to a single-threaded run\n",
    "    total_threads = 1\n",
    "    merge_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85741c5c-72ff-477e-928e-08d4e04cad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%env TOGETHER_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f7bb95-6eeb-42c9-89f5-7dfdf39a6d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.getenv(\"TOGETHER_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed55ce76-958f-4b5d-bcfd-73d326272df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = [\n",
    "    {\n",
    "        \"model\":\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",     \n",
    "        \"api_key\":os.getenv(\"TOGETHER_API_KEY\"),\n",
    "        \"max_tokens\": 20000\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e914d-94b2-4cfe-9f0a-a4c17ec88991",
   "metadata": {},
   "source": [
    "# Client Initiation with Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40fca58b-ec17-414a-aecb-bc5d1a779609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "client = Together(api_key=config_list[0][\"api_key\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "336e744c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#from GraphReasoning import *\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/orcd/home/002/istewart/orcd/pool/hypergraph/GraphReasoning_SG') #change functions here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ee71b51-cfa6-4dca-aa55-de6b48f19c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbatim=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac05b174-03ef-4443-b25b-fb571a23c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_data_dir = './CompositePDFs_marker' #place where you keep your markdown files\n",
    "data_dir='./GRAPHDATA_paper'    \n",
    "data_dir_output='./GRAPHDATA_OUTPUT_paper'\n",
    "\n",
    "max_tokens = config_list[0]['max_tokens']\n",
    "\n",
    "embedding_file='composite_LLAMA4_70b.pkl' #what your embedding file will be \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac48b9f9-0882-4fca-b69f-51887ed7f1d0",
   "metadata": {},
   "source": [
    "# Embedding the graph with Nomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369be73-4ae6-44d9-a779-22efb43d704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if total_threads == 1: ##merging mode: This block only runs if you're not distributing work across multiple threads.\n",
    "\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    embedding_tokenizer =''\n",
    "    embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "\n",
    "    from GraphReasoning import load_embeddings, save_embeddings, generate_hypernode_embeddings\n",
    "    \n",
    "    import hypernetx as hnx\n",
    "\n",
    "    import torch\n",
    "    generate_new_embeddings=True\n",
    "\n",
    "    if os.path.exists(f'{data_dir}/{embedding_file}'):\n",
    "        print('Found existing embedding file')\n",
    "        generate_new_embeddings=False\n",
    "    \n",
    "    # generate_new_embeddings=True\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if generate_new_embeddings:\n",
    "            H = hnx.Hypergraph({})\n",
    "            # Extract node IDs (will be empty here)\n",
    "            nodes = list(H.nodes)\n",
    "            # Generate embeddings for (new) nodes\n",
    "            node_embeddings = generate_hypernode_embeddings(\n",
    "                nodes,\n",
    "                embedding_tokenizer,\n",
    "                embedding_model,\n",
    "            )\n",
    "            # Save them\n",
    "            save_embeddings(node_embeddings, f'{data_dir}/{embedding_file}')\n",
    "        else:\n",
    "            # Load previously computed embeddings\n",
    "            node_embeddings = load_embeddings(f'{data_dir}/{embedding_file}')                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e892a7b-ec8c-4385-b429-d3c10c49ec52",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e47a90-fe76-4c50-8468-64dbcfaf76f7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Load dataset of papers\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "### Load all markdown files (dataset of papers) --- assumes each subfolder in doc_data_dir is named like a paper ID\n",
    "#doc_list becomes a list of full file paths to .md files\n",
    "\n",
    "doc_list=[]\n",
    "with os.scandir(f'{doc_data_dir}') as folders:\n",
    "    for folder in folders:\n",
    "        doc_list.append(f'{doc_data_dir}/{folder.name}/{folder.name}.md')\n",
    "\n",
    "doc_list=sorted(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393e1660",
   "metadata": {},
   "source": [
    "### Set up LLM client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f616e017-9eed-4354-8763-8f60221d5a0d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import instructor\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "import base64\n",
    "from pydantic import BaseModel\n",
    "from instructor import patch\n",
    "\n",
    "\n",
    "#new for hypergraph\n",
    "class Event(BaseModel):\n",
    "    source: List[str]\n",
    "    # target: str\n",
    "    target: List[str]\n",
    "    relation: str \n",
    "\n",
    "class HypergraphJSON(BaseModel):\n",
    "    events: List[Event]\n",
    "\n",
    "\n",
    "#Identifies phrases or terms that are potential nodes --> Decide the type of each node --> Determine which nodes are related, and what the relation is \n",
    "\n",
    "response_model = HypergraphJSON\n",
    "system_prompt = '''\n",
    "                 (\n",
    "                    \"You are a network ontology graph maker who extracts terms and their relations from a given context, using principles from category theory.\\n\\n\"\n",
    "\n",
    "                    \"You are provided with a context chunk (delimited by triple backticks: ```). Your task is to extract an ontology of terms mentioned in the context, representing key scientific concepts, systems, materials, and methods using well-defined, technical, and widely accepted terminology.\\n\\n\"\n",
    "\n",
    "                    \"Proceed step by step:\\n\"\n",
    "                    \"Thought 1: Traverse the text sentence by sentence. Identify key scientific terms, such as materials, methods, entities, systems, conditions, or acronyms. \\n\"\n",
    "                    \"    - Focus on extracting terms that are atomistic and domain-relevant.\\n\"\n",
    "                    \"    - Group modifiers (e.g., 'collagen scaffold') as one term if they form a recognized concept.\\n\\n\"\n",
    "\n",
    "                    \"Thought 2: Determine which terms are related to each other based on their co-occurrence in a sentence or paragraph.\\n\"\n",
    "                    \"    - A term may relate to multiple other terms.\\n\"\n",
    "                    \"    - Look for structural, functional, or procedural relationships.\\n\\n\"\n",
    "\n",
    "                    \"Thought 3: For each related group of terms, infer the scientific relationship between them.\\n\"\n",
    "                    \"    - Use category-theoretic relation names when possible, such as: 'is', 'has', 'acts on', 'used for', 'composed of', 'leads to'.\\n\"\n",
    "                    \"    - If 3 or more co-dependent entities relate to a shared target, use an n-ary relation with the source as a list.\\n\"\n",
    "                    \"    - If only 2 entities are involved, use a binary relation.\\n\\n\"\n",
    "\n",
    "                    \"Output Specification:\\n\"\n",
    "                    \"Return a JSON object with a single field: 'events'. Each event must contain:\\n\"\n",
    "                    \"- 'source': a string (for binary) or a list of entities (for n-ary)\\n\"\n",
    "                    \"- 'target': the main concept or object being acted upon or described\\n\"\n",
    "                    \"- 'relation': a concise, meaningful phrase describing the relation between source and target\\n\\n\"\n",
    "\n",
    "                    \"Important:\\n\"\n",
    "                    \"- Always preserve the original wording for technical terms.\\n\"\n",
    "                    \"- Do not invent entities or relations that are not implied in the text.\\n\"\n",
    "                    \"- Do not include any additional fields beyond 'source', 'target', and 'relation'.\\n\\n\"\n",
    "\n",
    "                    \"Examples:\\n\\n\"\n",
    "\n",
    "                    \"Binary relation:\\n\"\n",
    "                    \"{\\n\"\n",
    "                    \"  \\\"source\\\": \\\"hydrangea\\\",\\n\"\n",
    "                    \"  \\\"target\\\": \\\"flower\\\",\\n\"\n",
    "                    \"  \\\"relation\\\": \\\"is a type of\\\"\\n\"\n",
    "                    \"}\\n\\n\"\n",
    "\n",
    "                    \"N-ary relation:\\n\"\n",
    "                    \"{\\n\"\n",
    "                    \"  \\\"source\\\": [\\\"Sally\\\", \\\"Bob\\\", \\\"Julia\\\"],\\n\"\n",
    "                    \"  \\\"target\\\": \\\"paper 1\\\",\\n\"\n",
    "                    \"  \\\"relation\\\": \\\"are equal co-authors of\\\"\\n\"\n",
    "                    \"}\\n\\n\"\n",
    "\n",
    "                    \"Return a JSON object with this structure:\\n\"\n",
    "                    \"{\\n\"\n",
    "                    \"  \\\"events\\\": [\\n\"\n",
    "                    \"     {\\\"source\\\": ..., \\\"target\\\": ..., \\\"relation\\\": ...},\\n\"\n",
    "                    \"     {...},\\n\"\n",
    "                    \"     ...\\n\"\n",
    "                    \"  ]\\n\"\n",
    "                    \"}\"\n",
    "                    )\n",
    "'''\n",
    "\n",
    "def generate(system_prompt=system_prompt, \n",
    "             prompt=\"\",temperature=0.333,\n",
    "             max_tokens=config_list[0]['max_tokens'], response_model=HypergraphJSON, \n",
    "            ):     \n",
    "\n",
    "    if system_prompt==None:\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
    "        ]\n",
    "\n",
    "    else:\n",
    "        messages=[\n",
    "            {\"role\": \"system\",  \"content\": f\"{system_prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
    "        ]\n",
    "\n",
    "    # monkey patching: replacing or enhancing client.chat.completions.create\n",
    "    create = instructor.patch(     #instructor is a python library that wraps LLM responses and validates LLM output against a schema you define (via pydantic)\n",
    "        create=client.chat.completions.create,    #Automatically converts the response into a real Python object (not just a raw string or dictionary)\n",
    "        mode=instructor.Mode.JSON_SCHEMA,\n",
    "    ) \n",
    "    \n",
    "\n",
    "    return create(messages=messages,   \n",
    "                    model=config_list[0][\"model\"],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=0.333,\n",
    "                    response_model=response_model,\n",
    "                   )\n",
    "\n",
    "def image_to_base64_data_uri(file_path):\n",
    "    with open(file_path, \"rb\") as image_file:\n",
    "        base64_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        return f\"data:image/png;base64,{base64_data}\"\n",
    "\n",
    "def generate_figure(image, system_prompt=system_prompt, \n",
    "                prompt=\"\", temperature=0,\n",
    "                ):\n",
    "\n",
    "    pwd = os.getcwd()\n",
    "    image = image.split(pwd)[-1]\n",
    "    image=Path('.').glob(f'**/{image}', case_sensitive=False)\n",
    "    image = list(image)[0]\n",
    "\n",
    "    image_uri = image_to_base64_data_uri(image)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_uri}},\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image in detail please.\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "        \n",
    "    return create(messages=messages,   \n",
    "                    model=config_list[0][\"model\"],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=0.333,\n",
    "                    response_model=response_model,\n",
    "                   ).choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5cd7e7-b31d-4330-b374-762655a2f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'running on {thread_i}-th thread in totally {total_threads} threads') #double check threading "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a2225-cb47-47e8-9c6c-89e1d94b99eb",
   "metadata": {},
   "source": [
    "### Checkpoint Cell: Finds where merging left off with current_merged_i. Handles resuming graph merging in single-threaded mode\n",
    "\n",
    "#### 1. Finds the latest merged graph from previous runs.\n",
    "\n",
    "#### 2. Checks if it's valid by trying to load it.\n",
    "\n",
    "#### 3. If the graph or its parts are corrupted, it deletes them and rolls back to the previous merge index.\n",
    "\n",
    "#### 4. Sets current_merged_i so merging can resume cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113362c0-185b-4ae9-be5f-5be1317c7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, pickle\n",
    "\n",
    "INT_PREFIX = re.compile(r'^(\\d+)_')\n",
    "\n",
    "def extract_idx(path: str) -> int:\n",
    "    \"\"\"Return leading integer prefix before first underscore, else -1.\"\"\"\n",
    "    name = os.path.basename(path)\n",
    "    m = INT_PREFIX.match(name)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "# Directories\n",
    "doc_data_dir = './CompositePDFs_marker'\n",
    "data_dir = './GRAPHDATA_paper'\n",
    "data_dir_output = './GRAPHDATA_OUTPUT_paper'\n",
    "\n",
    "if total_threads == 1:  # merging mode\n",
    "    # 1) find all \"*_integrated.pkl\" files\n",
    "    merged_graph_list = sorted(\n",
    "        glob.glob(f'{data_dir_output}/*_integrated.pkl'),\n",
    "        reverse=True,\n",
    "        key=extract_idx\n",
    "    )\n",
    "\n",
    "    # 2) determine current merged index; fall back to 0 if none found\n",
    "    current_merged_i = extract_idx(merged_graph_list[0]) if merged_graph_list else 0\n",
    "\n",
    "    # 3) find any per-session files for the current index\n",
    "    last_graph = sorted(\n",
    "        glob.glob(f'{data_dir}/{current_merged_i}_*.pkl'),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    # 4) validate heads if both lists non-empty\n",
    "    if merged_graph_list and last_graph:\n",
    "        try:\n",
    "            with open(merged_graph_list[0], 'rb') as f:\n",
    "                _ = pickle.load(f)\n",
    "            with open(last_graph[0], 'rb') as f:\n",
    "                _ = pickle.load(f)\n",
    "        except Exception:\n",
    "            print(\"Validation failed; cleaning up corrupt files.\")\n",
    "            try:\n",
    "                os.remove(merged_graph_list[0])\n",
    "            except Exception:\n",
    "                pass\n",
    "            for fn in last_graph:\n",
    "                try:\n",
    "                    os.remove(fn)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            current_merged_i = max(0, current_merged_i - 1)\n",
    "    else:\n",
    "        if not merged_graph_list:\n",
    "            print(\"No integrated pickles; starting at 0.\")\n",
    "        else:\n",
    "            print(\"No per-session pickles to validate; skipping load check.\")\n",
    "\n",
    "    print(f\"Start merging from No. {current_merged_i}\")\n",
    "\n",
    "    # === Filter doc_list to remove missing markdown files ===\n",
    "    missing_log = os.path.join(data_dir_output, \"missing_markdown_folders.txt\")\n",
    "    \n",
    "    # clear old log once at the start of run\n",
    "    open(missing_log, \"w\").close()\n",
    "    \n",
    "    valid_doc_list = []\n",
    "    \n",
    "    for doc_path in doc_list:\n",
    "        # Check if the markdown file actually exists\n",
    "        if not os.path.exists(doc_path):\n",
    "            folder_path = os.path.dirname(doc_path)\n",
    "            print(f\"⚠️ Missing markdown for: {folder_path}, skipping.\")\n",
    "            with open(missing_log, \"a\") as logf:\n",
    "                logf.write(folder_path + \"\\n\")\n",
    "            continue\n",
    "        \n",
    "        valid_doc_list.append(doc_path)\n",
    "    \n",
    "    # Replace doc_list with filtered version\n",
    "    doc_list = valid_doc_list\n",
    "    print(f\"✓ Filtered doc_list: {len(doc_list)} valid documents (excluded missing markdown files)\")\n",
    "\n",
    "else:\n",
    "    current_merged_i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bfb33b-771c-46f2-acd7-26ea3f25e0a6",
   "metadata": {},
   "source": [
    "### Generate a Knowledge Graph (KG) from each document using an LLM + embedding pipeline, then (optionally) merge it into a global graph in merging mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53a91a-c933-48a8-b958-4241edeb4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import hypernetx as hnx\n",
    "from GraphReasoning import make_hypergraph_from_text, add_new_hypersubgraph_from_text, update_hypernode_embeddings\n",
    "from datetime import datetime\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import traceback\n",
    "\n",
    "\n",
    "# Initialize the \"global\" graph\n",
    "G = hnx.Hypergraph({})\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, doc in enumerate(doc_list):\n",
    "        # only process docs for this thread\n",
    "        if i % total_threads != thread_i:\n",
    "            continue\n",
    "        # skip already-merged docs\n",
    "        if i < current_merged_i:\n",
    "            continue\n",
    "\n",
    "        # extract title/doi and text\n",
    "        title = os.path.basename(doc).rsplit('.md', 1)[0]\n",
    "        doi = title\n",
    "        with open(doc, 'r') as f:\n",
    "            txt = f.read()\n",
    "\n",
    "        # define where this doc's subgraph lives\n",
    "        graph_root = f'{i}_{title[:100]}'\n",
    "        current_graph = os.path.join(data_dir, f'{graph_root}.pkl')\n",
    "\n",
    "        # Variable to store current document's sub_dfs\n",
    "        current_doc_sub_dfs = None\n",
    "        \n",
    "        # generate until the subgraph file appears\n",
    "        while not os.path.exists(current_graph):\n",
    "            print(f\"Generating KG for {i}: {title}\")\n",
    "            try:\n",
    "                if not isinstance(txt, str):\n",
    "                    print(\"Text is not a string:\", txt)\n",
    "                    break\n",
    "                now = datetime.now()\n",
    "                # FIX: Capture all return values including sub_dfs\n",
    "                current_graph, _, sub_dfs_pkl_path, current_doc_sub_dfs = make_hypergraph_from_text(\n",
    "                    txt, generate, generate_figure, image_list='',\n",
    "                    graph_root=graph_root,\n",
    "                    do_distill=False,\n",
    "                    do_relabel=False,\n",
    "                    chunk_size=10000, chunk_overlap=0,\n",
    "                    repeat_refine=0, verbatim=False,\n",
    "                    data_dir=data_dir,\n",
    "                )\n",
    "                print(\"Time:\", datetime.now() - now)\n",
    "                print(f\"[generation] Captured sub_dfs with {len(current_doc_sub_dfs) if current_doc_sub_dfs else 0} chunks\")\n",
    "            except Exception as e:\n",
    "                print(\"Error during KG generation:\", repr(e))\n",
    "                time.sleep(60)\n",
    "\n",
    "        # ── MERGING MODE ──\n",
    "        if total_threads == 1:\n",
    "\n",
    "            if i % merge_every == 0:\n",
    "                do_simplify_graph = True\n",
    "                size_threshold = 10\n",
    "            else:\n",
    "                do_simplify_graph = False\n",
    "                size_threshold = 0\n",
    "                \n",
    "            _hypergraph_pkl = os.path.join(data_dir_output, f'{graph_root}_integrated.pkl')\n",
    "            # skip if already merged\n",
    "            if os.path.exists(_hypergraph_pkl):\n",
    "                with open(_hypergraph_pkl, 'rb') as f:\n",
    "                    G = pickle.load(f)  \n",
    "                print(f\"[merge] already have integrated at {_hypergraph_pkl}\")\n",
    "                continue            \n",
    "            \n",
    "            now = datetime.now()\n",
    "            print(f\"[merge] about to merge paper {i}: will write {_hypergraph_pkl!r}\")\n",
    "\n",
    "            try:\n",
    "                graph_path = current_graph  # save the path\n",
    "                with open(graph_path, 'rb') as f:\n",
    "                    H0 = pickle.load(f)\n",
    "                current_graph = hnx.Hypergraph(\n",
    "                    H0.incidence_dict,\n",
    "                    edge_attr={'DOI': {eid: doi for eid in H0.incidence_dict}}\n",
    "                )\n",
    "                print(f\"[merge] loaded subgraph with {len(current_graph.edges)} edges\")\n",
    "            except Exception as e:\n",
    "                print(f\"[merge] failed loading/annotating {graph_path!r}: {e!r}\")\n",
    "                continue\n",
    "\n",
    "            ### FIX: Load cumulative sub_dfs\n",
    "            updated_path = os.path.join(data_dir_output, \"updated_sub_dfs.pkl\")\n",
    "            if os.path.exists(updated_path):\n",
    "                with open(updated_path, \"rb\") as f:\n",
    "                    sub_dfs = pickle.load(f)\n",
    "                print(f\"[merge] Loaded cumulative sub_dfs from {updated_path} with {len(sub_dfs)} existing chunks\")\n",
    "            else:\n",
    "                sub_dfs = []\n",
    "                print(f\"[merge] Starting fresh sub_dfs list\")\n",
    "\n",
    "            ### FIX: If generation happened (and wasn't skipped), add current doc's chunks\n",
    "            if current_doc_sub_dfs is None:\n",
    "                # Generation was skipped (file already existed), load from the saved pkl\n",
    "                sub_dfs_pkl_path = os.path.join(data_dir, \"original_sub_dfs.pkl\")\n",
    "                if os.path.exists(sub_dfs_pkl_path):\n",
    "                    with open(sub_dfs_pkl_path, \"rb\") as f:\n",
    "                        current_doc_sub_dfs = pickle.load(f)\n",
    "                    print(f\"[merge] Loaded current doc's sub_dfs from {sub_dfs_pkl_path}\")\n",
    "            \n",
    "            ### FIX: Append current document's chunks to cumulative list\n",
    "            if current_doc_sub_dfs:\n",
    "                if isinstance(current_doc_sub_dfs, list):\n",
    "                    sub_dfs.extend(current_doc_sub_dfs)\n",
    "                    print(f\"[merge] Added {len(current_doc_sub_dfs)} chunks from doc {i}. Total: {len(sub_dfs)} chunks\")\n",
    "                else:\n",
    "                    sub_dfs.append(current_doc_sub_dfs)\n",
    "                    print(f\"[merge] Added 1 chunk from doc {i}. Total: {len(sub_dfs)} chunks\")\n",
    "            else:\n",
    "                print(f\"[merge] WARNING: No sub_dfs found for doc {i}\")\n",
    "\n",
    "            \n",
    "            # perform the merge\n",
    "            integrated_path, G, _, node_embeddings, sub_dfs = add_new_hypersubgraph_from_text(\n",
    "                txt='',\n",
    "                node_embeddings=node_embeddings,\n",
    "                tokenizer=embedding_tokenizer,\n",
    "                model=embedding_model,\n",
    "                original_graph=G,\n",
    "                data_dir_output=data_dir_output,\n",
    "                graph_root=graph_root,\n",
    "                do_simplify_graph=do_simplify_graph,\n",
    "                do_relabel=False,\n",
    "                size_threshold=size_threshold,\n",
    "                do_update_node_embeddings=do_simplify_graph,\n",
    "                repeat_refine=0,\n",
    "                similarity_threshold=0.9,\n",
    "                do_Louvain_on_new_graph=False,\n",
    "                return_only_giant_component=False,\n",
    "                save_common_graph=False,\n",
    "                G_to_add=current_graph,\n",
    "                graph_pkl_to_add=None, \n",
    "                sub_dfs=sub_dfs,  # Now contains all previous + current doc's chunks\n",
    "                verbatim=True,\n",
    "            )\n",
    "\n",
    "            print(f\"[merge] After merge, sub_dfs contains {len(sub_dfs)} total chunks\")\n",
    "\n",
    "            # ── CONDITIONAL EMBEDDING UPDATE ──\n",
    "            doc_count = len(doc_list)\n",
    "            is_last_group = (doc_count < 100) or (i >= (doc_count // 100) * 100)\n",
    "\n",
    "            if is_last_group:\n",
    "                try:\n",
    "                    print(f\"[update] Performing embedding update for doc {i} (triggered by small or remainder group)\")\n",
    "                    node_embeddings = update_hypernode_embeddings(node_embeddings, G, embedding_tokenizer, embedding_model)\n",
    "                except Exception as e:\n",
    "                    print(f\"[update] Failed to update embeddings: {e!r}\")\n",
    "\n",
    "            # ── check consistency ──\n",
    "            graph_nodes = set(str(n) for n in G.nodes)\n",
    "            embedding_keys = set(str(k) for k in node_embeddings)\n",
    "        \n",
    "            missing = graph_nodes - embedding_keys\n",
    "            extra = embedding_keys - graph_nodes\n",
    "        \n",
    "            if not missing and not extra:\n",
    "                print(f\"[check] Embeddings are aligned with the graph nodes. Count = {len(graph_nodes)}\")\n",
    "            else:\n",
    "                print(f\"[check] Embedding mismatch detected:\")\n",
    "                if missing:\n",
    "                    print(f\"  - Missing embeddings for {len(missing)} nodes: {list(missing)[:5]}\")\n",
    "                if extra:\n",
    "                    print(f\"  - Extra embeddings for {len(extra)} nodes not in graph: {list(extra)[:5]}\")\n",
    "                        \n",
    "            #rename the very final last saved clean graph \n",
    "            is_last_doc = (i == len(doc_list) - 1)\n",
    "                        \n",
    "            if is_last_doc:\n",
    "                final_path = os.path.join(data_dir_output, \"final_graph.pkl\")\n",
    "                try:\n",
    "                    shutil.copyfile(integrated_path, final_path)\n",
    "                    print(f\"[merge] Final graph saved as: {final_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[merge] Failed to rename final graph: {e!r}\")\n",
    "            \n",
    "            try:\n",
    "                save_embeddings(node_embeddings, os.path.join(data_dir, embedding_file))\n",
    "                print(\"[merge] embeddings saved\")\n",
    "            except Exception as e:\n",
    "                print(f\"[merge] failed to save embeddings: {e!r}\")\n",
    "            \n",
    "            print(\"Merge elapsed time:\", datetime.now() - now)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
