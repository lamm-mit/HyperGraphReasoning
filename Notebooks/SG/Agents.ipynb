{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18aabf0c-77dc-4681-bd12-887598b7c54d",
   "metadata": {},
   "source": [
    "# MODELS and DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19de07ae-7c37-4ded-9071-d29146150e97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl: (48) An unknown option was passed in to libcurl\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:8080/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7062d4d-f41f-4f0e-b072-2e9e179ecd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/istewart/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "print(autogen.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e06ad9fb-692b-4d55-ae12-be00a9b55859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/istewart/orcd/pool/hypergraph/GraphReasoning_SG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4670a508-c911-41a8-9f54-8e8bc3877f21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84bb732b-38a9-461d-8ba4-20844e7e6e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import autogen, openai\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\":\"Llama3.3\",\n",
    "        \"base_url\": \"http://localhost:8080/v1\",\n",
    "        \"api_key\":\"NULL\",\n",
    "        \"max_tokens\": 40000\n",
    "    },\n",
    "]\n",
    "\n",
    "llm_config = {\n",
    "    \"cache_seed\": 9527,  # seed for caching and reproducibility\n",
    "    \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "    \"temperature\": 0,  # temperature for sampling\n",
    "    \"max_tokens\": 40000,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e52795f-d3c3-4e06-b6f6-d6c2026ba078",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='/home/istewart/orcd/pool/hypergraph/GraphReasoning_SG/Notebooks/SG/GRAPHDATA_paper' #contains the generated embedding file \n",
    "data_dir_output='/home/istewart/orcd/pool/hypergraph/GraphReasoning_SG/Notebooks/SG/GRAPHDATA_OUTPUT_paper' #contains all subgraphs \n",
    "embedding_file='composite_LLAMA4_70b.pkl' #embed\n",
    "\n",
    "max_tokens = config_list[0]['max_tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c2a09d-75ef-4bfa-ae84-35177df34b27",
   "metadata": {},
   "source": [
    "# GET EMBEDDING MODEL / TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "878198c2-5a76-4069-8e19-fb47f518f3dd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/istewart/orcd/pool/.conda/envs/LLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_tokenizer =''\n",
    "embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f56449b-434f-4293-9170-d9a3afee689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GraphReasoning import load_embeddings\n",
    "filename = f\"{data_dir}/{embedding_file}\"\n",
    "node_embeddings = load_embeddings(f'{data_dir}/{embedding_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a22e3e-e66b-4fb6-a273-c2841e865e49",
   "metadata": {},
   "source": [
    "# UPLOAD GRAPH G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d17312e-ad25-4cb2-b368-08f179d87834",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG loaded from /home/istewart/orcd/pool/hypergraph/GraphReasoning_SG/Notebooks/SG/GRAPHDATA_OUTPUT_paper/final_graph.pkl: None <class 'hypernetx.classes.hypergraph.Hypergraph'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "fname    = \"final_graph.pkl\" #the hypergraph pkl file (not the updated_sub_dfs) \n",
    "# fname    = \"simple_hypergraph_simplified.pkl\" #the hypergraph pkl file (not the updated_sub_dfs) \n",
    "\n",
    "fullpath = os.path.join(data_dir_output, fname)\n",
    "\n",
    "with open(fullpath, \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "print(f\"KG loaded from {fullpath}: {G}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a468e-8f3f-4b98-a66f-da2d13e142c3",
   "metadata": {},
   "source": [
    "# START GRAPHRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6dcbdb6-34a6-4c70-a577-b9166a80bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_nodes = set(str(n) for n in G.nodes)\n",
    "# embedding_keys = set(str(k) for k in node_embeddings.keys())\n",
    "# missing = graph_nodes - embedding_keys\n",
    "# extra   = embedding_keys - graph_nodes\n",
    "\n",
    "# if not missing and not extra:\n",
    "#     print(f\"Embeddings in sync. {len(graph_nodes)} nodes.\")\n",
    "# else:\n",
    "#     print(f\"Embedding mismatch detected.\")\n",
    "#     if missing:\n",
    "#         print(f\"  - Missing embeddings for {len(missing)} nodes: {list(missing)[:5]}\")\n",
    "#     if extra:\n",
    "#         print(f\"  - Extra embeddings for removed nodes: {len(extra)} nodes: {list(extra)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b65509-ff1c-4a50-86fa-389b06c64a78",
   "metadata": {},
   "source": [
    "# Count the number of tokens as a result of tokenizing the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1332931f-214f-4bb7-9456-6adfd88357bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_token_count_function(text, placeholder=''):\n",
    "    return len(embedding_tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf1a68c9-dafa-4ae1-a164-6b9249f824e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "from typing import Optional, Union, cast, TypeVar\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "from chromadb.api.types import EmbeddingFunction, Embeddings\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "from chromadb.config import Settings\n",
    "import torch #torch is used to move tokenized inputs to GPU, handling model outputs as tensors, performing .mean() and .detach model layers. \n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#write this to conform to ChromaDB's embedding function fromat \n",
    "Embeddable = Union[str, nx.DiGraph] #type alias literally meaning \"str or nx.DiGraph” be considered 'embeddable', allowing to accept more than one type of input. \n",
    "D = TypeVar(\"D\", bound=Embeddable, contravariant=True) #defines generic type variable called D (subtype of embeddable - either string or a directed graph but not both at same time) \n",
    "\n",
    "\n",
    "\n",
    "#define embedding function compatible with ChromaDB's API\n",
    "class TransformerEmbeddingFunction(EmbeddingFunction[D]):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_tokenizer,\n",
    "            embedding_model,\n",
    "            cache_dir: Optional[str] = None,\n",
    "    ):\n",
    "        try:\n",
    "            from transformers import AutoModel, AutoTokenizer\n",
    "            self._torch = importlib.import_module(\"torch\")\n",
    "\n",
    "            #here we are using our own embedding tokenizer and embedding model, but it can also be retrieved from hugging face transformers library \n",
    "            self._tokenizer = embedding_tokenizer #AutoTokenizer.from_pretrained(model_name)\n",
    "            self._model = embedding_model #AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda:0\", cache_dir=cache_dir)\n",
    "        except ImportError:\n",
    "            raise ValueError(\n",
    "                \"The transformers and/or pytorch python package is not installed. Please install it with \"\n",
    "                \"`pip install transformers` or `pip install torch`\"\n",
    "            )\n",
    "\n",
    "    \n",
    "    #Defines normalizing embedding vectors. Ensures the resulting embedding vector has length = 1 - This is important for cosine similarity in vector databases\n",
    "    @staticmethod\n",
    "    def _normalize(vector: npt.NDArray) -> npt.NDArray:\n",
    " \n",
    "        \"\"\"Normalizes a vector to unit length using L2 norm.\"\"\"\n",
    "        norm = np.linalg.norm(vector)\n",
    "        if norm == 0:\n",
    "            return vector\n",
    "        return vector / norm\n",
    "\n",
    "\n",
    "    \n",
    "    #tokenizes and embeds the input, gets sentence embeddings, normalizes for cosine similarity later and returns as list of lists to store in chromaDB. \n",
    "    def __call__(self, input: D) -> Embeddings:\n",
    "\n",
    "        if self._tokenizer:\n",
    "            inputs = self._tokenizer(\n",
    "                input, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "            ).to('cuda:0') #Tokenizes input returns it as a tensor and moves it to GPU, passes it through transformer model\n",
    "            outputs = self._model(**inputs) #Feeds the tokenized input into the transformer model\n",
    "            try:\n",
    "                embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()  # mean pooling, Takes the average of all token embeddings (basic way to get sentence embeddings)\n",
    "            except:\n",
    "                embeddings = outputs.hidden_states[-1].mean(dim=1).detach().to(torch.float).cpu().numpy() \n",
    "    \n",
    "            \n",
    "        else:\n",
    "            embeddings = self._model.encode(input) #If no tokenizer is set, assume the model has its own .encode() method, this supports other libraries like sentence-transformers! \n",
    "         \n",
    "        \n",
    "        return [e.tolist() for e in self._normalize(embeddings)] #normalizes each embedding and returns it as a list of lists, which is what ChromaDB expects for embeddings\n",
    "\n",
    "        #ChromaDB expects [[0.12, -0.45, 0.77, ...]] That is a list of embeddings, where each embedding is itself a list (a vector).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efda859a-7e21-416f-82cf-26d6ba6b5374",
   "metadata": {},
   "source": [
    "# Define Variable Embedding_function to embed with the predefined embedding tokenizer and embedding model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec8c4dc6-8d92-4c81-844c-ffc834098df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = TransformerEmbeddingFunction(embedding_tokenizer=embedding_tokenizer, embedding_model=embedding_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0383d3-4e5e-42f1-a1c8-2319086e2506",
   "metadata": {},
   "source": [
    "# GraphRAGAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66d20a14-cb7e-47f5-84c8-8f2b13242f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# from autogen import AssistantAgent, UserProxyAgent\n",
    "# from GraphReasoning import extract_keywords_to_nodes, find_shortest_path_hypersubgraph_between_nodes_local\n",
    "\n",
    "# --- LLM + OpenAI client ---\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- Autogen agents ---\n",
    "from autogen import AssistantAgent, UserProxyAgent, Agent\n",
    "\n",
    "# --- typing helpers ---\n",
    "from typing import Optional, Callable, Dict, List, Tuple, Any\n",
    "from typing import Literal\n",
    "\n",
    "# --- your custom graph reasoning functions ---\n",
    "from GraphReasoning import (\n",
    "    extract_keywords_to_nodes,\n",
    "    find_shortest_path_hypersubgraph_between_nodes_local,\n",
    "    collect_hyperentities, load_chunk_dfs,\n",
    ")\n",
    "\n",
    "# --- for IPython integration (used in your agent) ---\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "except ImportError:\n",
    "    def get_ipython():\n",
    "        return None\n",
    "\n",
    "# --- graph libraries ---\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# local_search(question, generate, graph, node_embeddings, embedding_tokenizer, embedding_model, N_samples=5, similarity_threshold=0.9)\n",
    "class llm:\n",
    "    def __init__(self, llm_config):\n",
    "        self.client = OpenAI(api_key=llm_config[\"api_key\"], #links to a serving client... in this case its actually llama CPP \n",
    "                             base_url=llm_config[\"base_url\"], #localhost URL to llama CPP \n",
    "                             )\n",
    "        self.model = llm_config[\"model\"] #stores model name\n",
    "        self.max_tokens = llm_config[\"max_tokens\"] #stores max token count \n",
    "        \n",
    "    def generate_cli(self, system_prompt=\"You are an expert in this field. Try your best to give a clear and concise answer.\", #default if nothing is given of the definition, tone of the model\n",
    "                           prompt=\"Hello world! I am\", temperature=0, #default if nothing is given, prompt input and temperature is 0 so determistic \n",
    "                           ):     \n",
    "        try:\n",
    "            if system_prompt==None: #if no sytem prompt is given, then only the user's message is included \n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "\n",
    "                ]\n",
    "\n",
    "            else:\n",
    "                messages=[\n",
    "                    {\"role\": \"system\",  \"content\": system_prompt}, #otherwise if it is given prepend it as a personaltiy \n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "\n",
    "                ]\n",
    "            result=self.client.chat.completions.create( #uses the openAI cient library to talk to our model \n",
    "                    #the folloiwing are generally the four things you need - model, message, temperature max tokens. \n",
    "                    model=self.model,\n",
    "                    messages=messages, #includes roles (users) , prompt, system prompt etc. \n",
    "                    temperature=temperature,\n",
    "                    max_tokens=self.max_tokens,\n",
    "                )\n",
    "\n",
    "            return result.choices[0].message.content #choices is the list of responses generated, there's usually only one hence the first index. message contains role+content, content is just the response. \n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "llm=llm(llm_config=config_list[0]) #autogen expects llm_config with it being a config list         \n",
    "generate = llm.generate_cli #set generate as a variable for calling the llm client using our cusom function generate_cli. \n",
    "\n",
    "\n",
    "class GraphRAGAgent(UserProxyAgent):\n",
    "    \"\"\"(In preview) The Graph Retrieval-Augmented User Proxy retrieves information from knowledge graphs based on the embedding\n",
    "    similarity, and sends them along with the question to this or next assistant\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name=\"GraphRAGChatAgent\",  # default set to RetrieveChatAgent\n",
    "        human_input_mode: Literal[\"ALWAYS\", \"NEVER\", \"TERMINATE\"] = \"NEVER\",\n",
    "        is_termination_msg: Optional[Callable[[Dict], bool]] = None, # setting defaults to none \n",
    "        generate=None, # setting defaults to none \n",
    "        node_embeddings=None, # setting defaults to none \n",
    "        embedding_tokenizer=None, # setting defaults to none \n",
    "        embedding_model=None, # setting defaults to none \n",
    "        retrieve_config: Optional[Dict] = None,  # config for the retrieve agent\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            human_input_mode=human_input_mode,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self._retrieve_config = {} if retrieve_config is None else retrieve_config\n",
    "        self._knowledge_graph = self._retrieve_config.get(\"knowledge_graph\", None)\n",
    "        self._n_results = self._retrieve_config.get(\"n_results\", 5)\n",
    "        self._distance_threshold = self._retrieve_config.get(\"distance_threshold\", 0.9)\n",
    "        self._intersection_threshold = self._retrieve_config.get(\"intersection_threshold\", 1)\n",
    "        self._k_paths = self._retrieve_config.get(\"k_paths\", 1)\n",
    "\n",
    "        \n",
    "        self.generate = generate\n",
    "        self.node_embeddings = node_embeddings\n",
    "        self.embedding_tokenizer = embedding_tokenizer\n",
    "        self.embedding_model = embedding_model\n",
    "        \n",
    "        self._ipython = get_ipython()\n",
    "        self._results = []  # the results of the current query\n",
    "        self._intermediate_answers = set()  # the intermediate answers\n",
    "        self.register_reply(Agent, GraphRAGAgent._generate_retrieve_user_reply, position=2)\n",
    "\n",
    "    def _reset(self, intermediate=False):\n",
    "        # self._doc_idx = -1  # the index of the current used doc\n",
    "        self._results = []  # the results of the current query\n",
    "        if not intermediate:\n",
    "            self._intermediate_answers = set()  # the intermediate answers\n",
    "            self._doc_contents = []  # the contents of the current used doc\n",
    "            self._doc_ids = []  # the ids of the current used doc\n",
    "            \n",
    "    def graphRAG(self, message):  \n",
    "        nodes = extract_keywords_to_nodes(message, self.generate, self.node_embeddings, self.embedding_tokenizer, self.embedding_model, self._n_results, similarity_threshold=self._distance_threshold, H=self._knowledge_graph)\n",
    "        subgraph, reports = find_shortest_path_hypersubgraph_between_nodes_local(self._knowledge_graph, nodes, s=self._intersection_threshold, k_paths=self._k_paths,)\n",
    "        chunk_to_df = load_chunk_dfs(\"/orcd/pool/007/istewart/hypergraph/GraphReasoning_SG/Notebooks/SG/GRAPHDATA_OUTPUT_paper/updated_sub_dfs.pkl\")\n",
    "        return collect_hyperentities(subgraph, reports, chunk_to_df=chunk_to_df)\n",
    "\n",
    "#Main function when a message is recieved, basically just calls graphRAG function above. \n",
    "    def _generate_retrieve_user_reply(\n",
    "        self,\n",
    "        messages: Optional[List[Dict]] = None,\n",
    "        sender: Optional[Agent] = None,\n",
    "        config: Optional[Any] = None,\n",
    "    ) -> Tuple[bool, Union[str, Dict, None]]:\n",
    "        \"\"\"In this function, we will update the context and reset the conversation based on different conditions.\n",
    "        We'll update the context and reset the conversation if update_context is True and either of the following:\n",
    "        \"\"\"\n",
    "        \n",
    "        self._reset(intermediate=True) #reset conversation \n",
    "\n",
    "        relationships = self.graphRAG(self._oai_messages[sender][-1]) #get out the relationship from graph \n",
    "                \n",
    "        final_response = f\"Please consider the following relationships of the knowledge related to the question and make your response: {relationships}\"\n",
    "\n",
    "        self.clear_history(sender)\n",
    "        sender.clear_history(self)        \n",
    "\n",
    "        return True, final_response # self._generate_message(doc_contents, problem=_message, task=self._task)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1ad5e-0f8b-4f5d-82ac-f4e32e2f9f10",
   "metadata": {},
   "source": [
    "# Experiments on Path Finding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af78b347-777b-4a92-9c83-bea561beba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Hypergraph path filtering parameters ----\n",
    "intersection_threshold = 2\n",
    "k_paths = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82aa430-f9b4-4ca6-822a-0c92a972bff9",
   "metadata": {},
   "source": [
    "# Set up multiagent system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24d55078-17a8-4cd9-a93f-69ead7e22d3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"Admin\",\n",
    "    system_message=\"A human admin. Interact with the engineer to discuss the QA result.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False, #disables the ability to execute Python code via an LLM, ensuring it's purely conversational.\n",
    "    is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\", \"\").replace(\"*\", \"\").rstrip(),  #stops conversation if the word TERMINATE appears \n",
    "    # is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    ")\n",
    "\n",
    "graph_rag_agent = GraphRAGAgent(\n",
    "    name=\"graph_rag_agent\",\n",
    "    system_message=\"\"\"RAG agent. \n",
    "\"\"\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    "    generate = generate,\n",
    "    node_embeddings = node_embeddings,\n",
    "    embedding_tokenizer = embedding_tokenizer,\n",
    "    embedding_model = embedding_model,\n",
    "    retrieve_config={\n",
    "        \"n_results\": 5,\n",
    "        \"knowledge_graph\": G,\n",
    "        \"distance_threshold\": 1.5,\n",
    "        \"intersection_threshold\": intersection_threshold,\n",
    "        \"k_paths\": k_paths,\n",
    "    },\n",
    "    llm_config=llm_config,\n",
    "    is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\", \"\").replace(\"*\", \"\").rstrip(),\n",
    ")\n",
    "\n",
    "engineer = autogen.AssistantAgent(\n",
    "    name=\"Engineer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"Engineer with scientific backgrounds. Don't write code.\n",
    "Start your response with: QUESTION: ...? \\n ANSWER: ... .\n",
    "You should always use the information you recieve from the other agents and don't make assumption. You should keep references when you use the provided information from another agent\n",
    "Write your answer strictly in academic style with citations such as '<something true> [1]' and a references section with [1] <REFERENCE TITLE>: <reasons> and following the number in all your answers to make sure your citation is not overlapping.\n",
    "Don't ever cite any sources that are not from the information you have. If you have an idea that is hypothetical, only mark it in your response.\n",
    "Don't indirect cite the reference from the source texts.\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "hypothesizer = autogen.AssistantAgent(\n",
    "    name=\"Engineer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"Creative Hypothesizer agent. Based on insights from previous agents about mechanistic combinations, suggest a plausible experiment or new material or composite that could reveal new insight. \n",
    "Add \"\\nCLEAR HISTORY graph_rag_agent\" at the end of your reply\n",
    "\"\"\",\n",
    "    is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\", \"\").replace(\"*\", \"\").rstrip(), \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095eb27e-c8b5-47b8-8aec-8643c54f99a5",
   "metadata": {},
   "source": [
    "# Define Groupchat Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e894196b-c219-4130-bcc4-11f6c82a6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents = [user_proxy, graph_rag_agent, planner, hgraph_rag_agent, engineer, critic, summarizer]\n",
    "agents = [user_proxy, graph_rag_agent, engineer, hypothesizer]\n",
    "def graphRAG_speaker_selection_func(last_speaker: Agent, groupchat: autogen.GroupChat):\n",
    "    \"\"\"Define a customized speaker selection function.\n",
    "    A recommended way is to define a transition for each speaker in the groupchat.\n",
    "\n",
    "    Returns:\n",
    "        Return an `Agent` class or a string from ['auto', 'manual', 'random', 'round_robin'] to select a default method to use.\n",
    "    \"\"\"\n",
    "    messages = groupchat.messages #messages is a built-in property of the GroupChat object in Autogen. It stores the entire conversation history as a list of dictionaries, where each dict is a message\n",
    "\n",
    "    if last_speaker is user_proxy: # we set some rules like if the last_speaker is a user_proxy then the next agent should always be graph_rag_agent. \n",
    "        return graph_rag_agent\n",
    "\n",
    "    if last_speaker is graph_rag_agent: #if the last speaker is the graph_rag_agent then the next speaker must be an engineer \n",
    "        return engineer\n",
    "\n",
    "    if last_speaker is engineer: \n",
    "        return hypothesizer\n",
    "    \n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=agents,\n",
    "    messages=[], #initialize storage of messages \n",
    "    max_round=100,\n",
    "    speaker_selection_method=graphRAG_speaker_selection_func, #we defined our own speaker selection, but could also be 'round_robin' \n",
    "    enable_clear_history=True, # enables message clearing with in-conversation commands like CLEAR HISTORY graph_rag_agent\n",
    "    # speaker_selection_method='round_robin',\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(groupchat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62da12c0-2553-4d40-b978-4142d95a2d01",
   "metadata": {},
   "source": [
    "# Run query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fb9666b-834b-4d03-91ff-747d2e44325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "#delete cache history \n",
    "try:\n",
    "    shutil.rmtree('.cache') #force reloading model weights and tokenizer in case any changes were made in previous cells  \n",
    "except:\n",
    "    pass #move on if errors like .cache doesn't exist, permission errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a5b8a5a-17ec-491b-b75f-74d0d5a0d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "q=[]\n",
    "q.append('''\n",
    "how can hydrogel mechanistically relate to PCL? \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0c1c3ba-f86e-4ac0-960d-2c1d9196bd00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "how can hydrogel mechanistically relate to PCL? \n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: graph_rag_agent\n",
      "\u001b[0m\n",
      "\n",
      "RAW LLM OUTPUT: {\"keywords\": [\"hydrogel\", \"pcl\"]}\n",
      "Extracted keywords: ['hydrogel', 'pcl']\n",
      "Found matched nodes in embeddings: [np.str_('hydrogel'), np.str_('PCL')]\n",
      "\u001b[33mgraph_rag_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "Please consider the following relationships of the knowledge related to the question and make your response: ['chitosan, collagen compose hydrogel.', 'PCL, chitosan, collagen, gelatin form scaffolds.']\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Engineer\n",
      "\u001b[0m\n",
      "[autogen.oai.client: 12-03 01:52:27] {351} WARNING - Model Llama3.3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "QUESTION: How can hydrogel mechanistically relate to PCL? \n",
      "ANSWER: Hydrogel can mechanistically relate to PCL through the common components used in forming scaffolds, such as chitosan and collagen [1]. Since chitosan and collagen are known to compose hydrogels [2], and these same biomaterials, along with PCL, can form scaffolds [3], it suggests a potential intersection or compatibility between hydrogel matrices and PCL-based structures. This relationship could be exploited in tissue engineering applications where the combination of mechanical strength provided by PCL and the bioactive properties of hydrogels composed of chitosan and collagen could enhance scaffold performance [4]. \n",
      "\n",
      "References:\n",
      "[1] The formation of scaffolds using PCL, chitosan, collagen, and gelatin indicates a potential for material compatibility: PCL, chitosan, collagen, gelatin form scaffolds.\n",
      "[2] Chitosan and collagen are components of hydrogels: chitosan, collagen compose hydrogel.\n",
      "[3] The use of similar biomaterials in both hydrogels and PCL-based scaffolds suggests a mechanistic relationship: PCL, chitosan, collagen, gelatin form scaffolds, and chitosan, collagen compose hydrogel.\n",
      "[4] Hypothetically, combining the properties of hydrogels with those of PCL could lead to improved tissue engineering outcomes, though this would require further investigation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Engineer\n",
      "\u001b[0m\n",
      "[autogen.oai.client: 12-03 01:53:01] {351} WARNING - Model Llama3.3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "To further explore the mechanistic relationship between hydrogels and PCL, an experiment could be designed to create a composite scaffold that integrates the strengths of both materials. Here's a potential approach:\n",
      "\n",
      "1. **Fabrication of Composite Scaffolds**: Develop a method to combine PCL with a hydrogel composed of chitosan and collagen. This could involve electrospinning PCL fibers and then encapsulating them within a hydrogel matrix, or vice versa, where hydrogel microspheres are embedded within a PCL scaffold.\n",
      "\n",
      "2. **Mechanical and Biological Characterization**: Evaluate the mechanical properties (e.g., tensile strength, elasticity) of the composite scaffolds in comparison to pure PCL scaffolds and hydrogels. Additionally, assess their biological performance by culturing cells relevant to tissue engineering applications (e.g., osteoblasts for bone tissue engineering) on these scaffolds.\n",
      "\n",
      "3. **Degradation Studies**: Since both PCL and the components of hydrogels (chitosan and collagen) have different degradation rates, it's crucial to study how the composite scaffold degrades over time under physiological conditions. This would provide insights into its potential longevity and biocompatibility in vivo.\n",
      "\n",
      "4. **In Vivo Studies**: For a more comprehensive understanding, conduct animal studies where these composite scaffolds are implanted to assess their integration with host tissue, their ability to support tissue regeneration, and any adverse reactions.\n",
      "\n",
      "By exploring the intersection of hydrogels and PCL through such experiments, researchers could uncover new insights into creating advanced biomaterials for tissue engineering that combine the beneficial properties of both materials, potentially leading to improved outcomes in regenerative medicine.\n",
      "\n",
      "CLEAR HISTORY graph_rag_agent\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No eligible speaker found. Terminating the conversation.\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for q_ in q:\n",
    "    result.append(\n",
    "    user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=f'''{q_}\n",
    "''',)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e6168-3327-4669-a3b9-2b855a52d112",
   "metadata": {},
   "source": [
    "# SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc6da64-37d5-46ee-bbb6-d56ac73637d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved filtered notebook cells to Experiments_Nov23/hydrogel_PCL_k1_2IT_hypothesis.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nbformat\n",
    "from nbconvert import HTMLExporter\n",
    "def export_filtered_notebook_cell(notebook_path, search_snippet, out_dir=\"Experiments_Nov23\", out_filename=\"filtered_notebook.html\"):\n",
    "    # Load notebook\n",
    "    nb = nbformat.read(notebook_path, as_version=4)\n",
    "\n",
    "    # Find cells containing the snippet\n",
    "    matched_cells = [cell for cell in nb.cells if cell.cell_type == \"code\" and search_snippet in cell.source]\n",
    "\n",
    "    if not matched_cells:\n",
    "        raise ValueError(f\"No cell found containing: {search_snippet}\")\n",
    "\n",
    "    # Keep only matched cells\n",
    "    nb.cells = matched_cells\n",
    "\n",
    "    # Export to HTML with lab template (white theme)\n",
    "    exporter = HTMLExporter(template_name=\"lab\")\n",
    "    body, _resources = exporter.from_notebook_node(nb)\n",
    "\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Save HTML\n",
    "    out_html = os.path.join(out_dir, out_filename)\n",
    "    with open(out_html, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(body)\n",
    "\n",
    "    print(f\"✅ Saved filtered notebook cells to {out_html}\")\n",
    "\n",
    "export_filtered_notebook_cell(\n",
    "    notebook_path=\"/home/istewart/orcd/pool/hypergraph/GraphReasoning_SG/Notebooks/SG/An_LLM_hypergraph.ipynb\",\n",
    "    search_snippet=\"result = []\",\n",
    "    out_dir=\"Experiments_Nov23\",\n",
    "    out_filename=\"hydrogel_PCL_k1_2IT_hypothesis.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29f76f-72b6-41f6-94e8-ce891a6dda5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
